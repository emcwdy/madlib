# coding=utf-8

"""
@file dt.py_in

@breif common functions written in PL/Python shared by C4.5 and RF
@namespace dt
"""
import datetime
import math
import plpy
import dt_preproc as preproc
import dt_utility as util

def __delete_traininginfo(madlib_schema, tree_table):
    """
    @brief Remove the trained tree from training info table.

    @param tree_table    The full name of the tree table.
    """
    stmt = """
           DELETE FROM {madlib_schema}.training_info
           WHERE result_table_oid = '{tree_table}'::regclass;
           """.format(
               madlib_schema = madlib_schema,
               tree_table = tree_table
           )
    plpy.execute(stmt)


def __insert_into_traininginfo(
    madlib_schema               ,
    classifier_table_name       ,
    result_table_name           ,
    training_table_name         ,
    training_metatable_name     ,
    training_encoded_table_name ,
    validation_table_name       ,
    how2handle_missing_value    ,
    split_criterion             ,
    sampling_percentage         ,
    num_features_chosen         ,
    num_trees                   
):
 
    """
    @brief Insert the trained tree into training info table.

    @param classifier_table_name         The name of the classifier.
    @param result_table_name             The full name of the training result 
                                         table.
    @param training_table_name           The full name of the training table.
    @param training_metatable_name       The full name of metatable.
    @param training_encoded_table_name   The full name of the encoded table.
    @param validation_table_name         The full name of the validation table.
    @param how2handle_missing_value      The name of the routine to process 
                                         unknown values.
    @param split_criterion               The name of split criterion.
    @param sampling_percentage           The percentage of bootstrap samples 
                                         size in training dataset.
    @param num_features_chosen           The number of features to split on 
                                         each tree node.
    @param num_trees                     The number of trees after completed.
    """
    if validation_table_name is None :
        val_tbl_name_sql = "NULL::regclass,"
    else :
        val_tbl_name_sql = "'{validation_table_name}'::regclass,".format(
                                validation_table_name = validation_table_name
                           )
    
    stmt = """
           INSERT INTO {madlib_schema}.training_info VALUES (
               '{classifier_table_name}',
               '{result_table_name}'::regclass,
               '{training_table_name}'::regclass,
               '{training_metatable_name}'::regclass,
               '{training_encoded_table_name}'::regclass,
               {val_tbl_name_sql}
               '{how2handle_missing_value}',
               '{split_criterion}',
               {sampling_percentage}, 
               {num_features_chosen},
               {num_trees}
           )
           """.format (
               madlib_schema = madlib_schema,
               classifier_table_name = classifier_table_name,
               result_table_name = result_table_name,
               training_table_name = training_table_name,
               training_metatable_name = training_metatable_name,
               training_encoded_table_name = training_encoded_table_name,
               val_tbl_name_sql = val_tbl_name_sql,
               how2handle_missing_value = how2handle_missing_value,
               split_criterion = split_criterion,
               sampling_percentage = sampling_percentage,
               num_features_chosen = num_features_chosen,
               num_trees = num_trees
           )
    plpy.execute(stmt)
   

def __get_encode_table_name(madlib_schema, tree_table):
    """
    @brief Get the encoding table name by tree table name.

    @param tree_table    The full name of the tree table.

    @return The full name of the encoded table.
    """
    stmt = """
           SELECT {madlib_schema}.__regclass_to_text
           (
               training_encoded_table_oid
           ) AS encode_table_name
           FROM {madlib_schema}.training_info
           WHERE result_table_oid = '{tree_table}'::regclass
           """.format(
               madlib_schema = madlib_schema,
               tree_table = tree_table
           ) 
    encoded_table_name_t = plpy.execute(stmt)
    
    return util.__get_query_value(encoded_table_name_t, "encode_table_name")


def __is_valid_enc_table(madlib_schema, enc_tbl_name):
    """
    @brief Test if the given table is a valid encoded one.
           A valid encoded table has the following characteristic:
           + Its OID is in the column "training_encoded_table_oid"
             of training_info table.
           + It has 5 columns, whose names are id, fid, fval,
             is_cont and class.
           + The types of the 5 columns are BIGINT, INT, FLOAT8
             BOOL and INT.

    @param enc_tbl_name    The full name of the encoded table.

    @return True if the given table is a valid encoded one.
            False if it's an invalid encoded table.
    """
    ret = False
    
    # test if the table is in the training_info table
    stmt = """
           SELECT count(*) AS count
           FROM {madlib_schema}.training_info
           WHERE {madlib_schema}.__regclass_to_text
                     (training_encoded_table_oid) = '{enc_tbl_name}'
           """.format(
               madlib_schema = madlib_schema, 
               enc_tbl_name = enc_tbl_name
           )

    t = plpy.execute(stmt)
    num_enc_table = util.__get_query_value(t, "count")

    # test if the name and the type of a column are valid or not
    stmt = """
           SELECT count(*) AS count
           FROM pg_attribute
           WHERE attrelid= '{enc_tbl_name}'::regclass::oid AND
                 attnum > 0 AND
                 not attisdropped AND
                 attname in ('id', 'fid', 'fval', 'is_cont', 'class') AND
                 atttypid in ('int8'::regtype, 'int'::regtype, 
                     'float8'::regtype, 'bool'::regtype, 'int'::regtype)
           """.format(enc_tbl_name = enc_tbl_name)
    t = plpy.execute(stmt)
    num_cols = util.__get_query_value(t, "count")

    if (num_enc_table > 0) and (num_cols == 5) :
        ret = True

    return ret


def __get_metatable_name(madlib_schema, tree_table):
    """
    @brief Get the meta table name by the tree table name.

    @param tree_table    The full name of the tree table.

    @return The full name of the metatable. 
    """
    util.__assert_table(madlib_schema, tree_table, True)
    
    util.__assert_table(
        madlib_schema, 
        str(madlib_schema) + ".training_info", True
    )

    stmt = """
           SELECT {madlib_schema}.__regclass_to_text
               (training_metatable_oid) AS metatable_name
           FROM {madlib_schema}.training_info
           WHERE result_table_oid = '{tree_table}'::regclass
           """.format(
               madlib_schema = madlib_schema,
               tree_table = tree_table
           )
    metatable_name_t = plpy.execute(stmt)

    return util.__get_query_value(metatable_name_t, "metatable_name")


def __get_routine_id(madlib_schema, tree_table):
    """
    @brief Get the unknown values processing routine id.

    @param tree_table    The full name of the tree table.

    @return The encoded missing value processing routine id.
    """
    name = __get_routine_name(madlib_schema, tree_table)
    
    if name == "ignore" :
        return 1
    elif name == "explicit" :
        return 2
    else:
        plpy.error("__get_routine_id:" + name)


def __get_routine_name(madlib_schema, tree_table):
    """
    @brief Get the unknown values processing routine name.
           The valid routine name is 'ignore' or 'explicit'.

    @param tree_table    The full name of the tree table.

    @return The encoded missing value processing routine name.
    """
    util.__assert_table(
        madlib_schema,
        madlib_schema + ".training_info",
        True
    )

    stmt = """
           SELECT how2handle_missing_value AS name
           FROM   {madlib_schema}.training_info
           WHERE  result_table_oid = '{tree_table}'::regclass
           """.format(
               madlib_schema = madlib_schema,
               tree_table = tree_table
           )
    name_t = plpy.execute(stmt)
 
    return util.__get_query_value(name_t, "name")


def __get_tree_table_name(madlib_schema, enc_table_name):
    """
    @brief Get the name of the tree table from the encoded table name.

    @param enc_table_name  The encoded table name.

    @return The full name of the tree table.
    """ 
    curstmt = """
              SELECT {madlib_schema}.__regclass_to_text
                  (result_table_oid::regclass) AS name
              FROM {madlib_schema}.training_info
              WHERE training_encoded_table_oid = 
                  '{enc_table_name}'::regclass
              LIMIT 1
              """.format(
                  enc_table_name = enc_table_name, 
                  madlib_schema = madlib_schema
              )  
    t = plpy.execute(curstmt)
    
    return util.__get_query_value(t, "name")


def __get_features_of_nodes (
    madlib_schema         ,
    nid_table_name        ,
    result_table_name     ,
    num_chosen_fids       ,
    total_num_fids        ,
    verbosity             
):
    """
    @brief Retrieve the selected features for a node. We will create a table,
           named sf_association, to store the association between selected 
           feature IDs and node IDs.
    @param nid_table_name    The full name of the table which contains all the
                             node IDs.
    @param result_table_name The full name of the table which contains the 
                             parent discrete features for each node.
    @param num_chosen_fids   The number of feature IDs will be chosen for a
                             node.
    @param total_num_fids    The total number of feature IDs, total_num_fids
                             >= num_chosen_fids.
                             If num_chosen_fids < total_num_fids, then we will
                             randomly select num_chosen_fids features from all
                             the features. Otherwise, we will return all the
                             features exception they belong to the parent 
                             discrete features for a node.
    @param verbosity         > 0 means this function runs in verbose mode.

    @return An constant string for the association table name.
    """
    """
    The sf_association table records which features are used
    for finding the best split for a node.
    It has two columns:
         nid -- The id of a node.
         fid -- The id of a feature.
    """
    plpy.execute("TRUNCATE sf_assoc")

    curstmt = """
              INSERT INTO sf_assoc(nid, fid)
              SELECT
                  nid,
                  unnest
                  (
                      {madlib_schema}.__dt_get_node_split_fids
                      (
                          {num_chosen_fids},
                          {total_num_fids},
                          nid,
                          dp_ids
                      )
                  ) as fid
              FROM
              (
                  SELECT nid, dp_ids
                  FROM {nid_table_name} s1, {result_table_name} s2
                  WHERE s1.nid = s2.id
                  GROUP BY nid, dp_ids
              ) t
              """.format (
                  madlib_schema = madlib_schema,
                  num_chosen_fids = int(num_chosen_fids), #maybe numeric or float
                  total_num_fids = total_num_fids,
                  nid_table_name = nid_table_name,
                  result_table_name = result_table_name
              )
   
    if verbosity > 0:
        plpy.info("build sample feature association stmt: " + curstmt)

    plpy.execute(curstmt)

    # we return an constant string for the association table name
    return 'sf_assoc'


def __gen_acc(
    madlib_schema           ,
    encoded_table_name      ,
    metatable_name          ,
    result_table_name       ,
    tr_table_name           ,
    sf_table_name           ,
    num_featrue_try         ,
    num_classes             ,
    sampling_needed         ,
    verbosity               
):
    """
    @brief Generate the ACC for current leaf nodes.

    @param encoded_table_name    The full name of the encoded table for the
                                 training table.
    @param metatable_name        The full name of the metatable contains the
                                 relevant information of the input table.
    @param result_table_name     The full name of the training result table.
    @param num_featrue_try       The number of features will be chosen per node.
    @param num_classes           Total number of classes in training set.
    @param verbosity             > 0 means this function runs in verbose mode.

    @return The time information for generating ACC.
    """
    num_fids = 1
    select_stmt = ""
    class ret : pass

    begin_calc_pre = datetime.datetime.now()

    # get the number of features
    curstmt = """
              SELECT COUNT(id) AS count
              FROM {metatable_name}
              WHERE column_type = 'f'
              """.format(metatable_name = metatable_name)
    
    t = plpy.execute(curstmt)
    num_fids = util.__get_query_value(t, "count")

    # preprocessing time
    ret.calc_pre_time = datetime.datetime.now() - begin_calc_pre
    begin_calc_acc = datetime.datetime.now()

    if sampling_needed :
        __get_features_of_nodes (
            madlib_schema,
            tr_table_name,
            result_table_name,
            num_featrue_try,
            num_fids,
            verbosity
        )
  
        select_stmt = """
                      SELECT tr.tid, tr.nid, ed.fid, ed.fval, ed.is_cont,
                          ed.class, sum(weight) as count
                      FROM {encoded_table_name} ed, {tr_table_name} tr,
                          {sf_table_name} sf
                      WHERE tr.nid = sf.nid AND ed.fid = sf.fid AND 
                          ed.id = tr.id
                      GROUP BY tr.tid, tr.nid, ed.fid, ed.fval,
                          ed.is_cont, ed.class
                      """.format (
                          encoded_table_name = encoded_table_name,
                          tr_table_name = tr_table_name,
                          sf_table_name = sf_table_name
                      )                     
    else :
        select_stmt = """
                      SELECT tr.tid, tr.nid, ed.fid, ed.fval, ed.is_cont,
                          ed.class, sum(weight) as count
                      FROM {encoded_table_name} ed, {tr_table_name} tr
                      WHERE ed.id = tr.id
                      GROUP BY tr.tid, tr.nid, ed.fid, ed.fval,
                          ed.is_cont, ed.class
                      """.format(
                          encoded_table_name = encoded_table_name, 
                          tr_table_name = tr_table_name
                      )
    
    plpy.execute("DROP TABLE IF EXISTS training_instance_aux;")

    curstmt = """
              CREATE TEMP TABLE training_instance_aux AS
              SELECT tid, nid, fid, fval, is_cont,
                  {madlib_schema}.__dt_acc_count_aggr
                  (
                      {num_classes},count::BIGINT,class::INT
                  ) AS count
              FROM
              (
                  {select_stmt}
              ) l
              GROUP BY tid,nid,fid, fval,is_cont
              m4_ifdef(`__GREENPLUM__', `DISTRIBUTED BY (fid, fval)')
              """.format(
                  num_classes = num_classes, 
                  select_stmt = select_stmt,
                  madlib_schema = madlib_schema
              )

    if verbosity > 0 :
        plpy.info(curstmt)

    plpy.execute(curstmt)
    
    ret.calc_acc_time = datetime.datetime.now() - begin_calc_acc

    return ret


def __find_best_split(
    madlib_schema,
    table_name,
    confidence_level,
    feature_table_name,
    split_criterion,
    continue_grow,
    output_table,
    h2hmv_routine_id,
    num_classes
):
    """
    @brief This function find the best split and return the information.

    @param table_name          The name of the table containing the training
                               set.
    @param confidence_level    This parameter is used by the 'Error-Based 
                               Pruning'.
                               Please refer to the paper for detailed 
                               definition.
                               The paper's name is 'Error-Based Pruning of 
                               Decision
                               Trees Grown on Very Large Data Sets Can Work!'.
    @param feature_table_name  Is is the name of one internal table, 
                               which contain meta data for each feature.
    @param sp_criterion        It defines the split criterion to be used.
                               (1- information gain. 2- gain ratio. 3- gini).
    @param continue_gow        It specifies whether we should still grow the tree
                               on the selected branch.
    @param output_table        It specifies the table used to store the chosen 
                               splits.
                               1 ignore, 2 explicit.
    """
    begin_func_exec = datetime.datetime.now()
    select_stmt = ""
   
    if h2hmv_routine_id == 1 :
        # For ignore, we need the true size of nodes to handle the missing 
        # values.
        select_stmt = """
                      SELECT t1.tid, t1.nid, t1.fid, t1.total, 
                          t2.node_size::BIGINT
                      FROM
                      (
                          SELECT tid, nid, fid, 
                              m4_ifdef(`__GREENPLUM__', `sum(count)',
                                  `{madlib_schema}.__bigint_array_sum(count)') 
                              AS total
                          FROM training_instance_aux
                          GROUP BY tid, nid, fid
                      ) t1 INNER JOIN node_size_aux t2
                      ON t1.tid=t2.tid AND t1.nid=t2.nid
                      """.format(madlib_schema = madlib_schema)
    else :
        # For explicit, the calculated node size from the aggregation is correct.
        # We can set NULL, which denotes we can safely use the counted value.
        select_stmt = """
                      SELECT tid, nid, fid,
                          m4_ifdef(`__GREENPLUM__', `sum(count)', 
                              `{madlib_schema}.__bigint_array_sum(count)') 
                          AS total,
                           NULL::BIGINT AS node_size
                      FROM training_instance_aux
                      GROUP BY tid, nid, fid
                      """.format(madlib_schema = madlib_schema)
      
    """   
    This table is used to store information for the calculated best split
   
    tid                  The ID of the tree.
    node_id              The ID of one node in the specified tree.
    feature              The ID of the selected feature.
    probability          The predicted probability of our chosen class.
    max_class            The ID of the class chosen by the algorithm.
    max_scv              The maximum split criterion value.
    live                 1- For the chosen split, we should split further.
                         0- For the chosen split, we shouldn't split further.
    ebp_coeff            total error for error-based pruning.
    is_cont              whether the selected feature is continuous.
    split_value          If the selected feature is continuous, it specifies
                         the split value. Otherwise, it is of no use.
    distinct_features    The number of distinct values for the selected feature.
    node_size            The size of this tree node.
    """   

    plpy.execute("DROP TABLE IF EXISTS " + output_table)

    plpy.execute(
        """
        CREATE TEMP TABLE {output_table}
        (
            tid                 INT,
            node_id             INT,
            feature             INT,
            probability         FLOAT,
            max_class           INTEGER,
            max_scv             FLOAT,
            live                INT,
            ebp_coeff           FLOAT,
            is_cont             BOOLEAN,
            split_value         FLOAT,
            distinct_features   INT,
            node_size           INT
        ) m4_ifdef(`__GREENPLUM__', `DISTRIBUTED BY (node_id)');
        """.format(output_table = output_table)
    )

    plpy.execute("DROP TABLE IF EXISTS tmp_best_table")

    curstmt = """
              INSERT INTO {output_table}
              SELECT tid, nid, best_scv[6], best_scv[4], best_scv[3],
                  best_scv[1],
                  CASE WHEN (best_scv[1] < 1e-9   OR
                      best_scv[4] > 1-1e-9 OR {continue_grow} <= 0 ) THEN
                      0
                  ELSE
                      1
                  END AS live,
                  {madlib_schema}.__ebp_calc_errors
                  (
                      best_scv[5], 
                      best_scv[4], 
                      {confidence_level}
                  ) AS ebp_coeff,
                  o2.is_cont,
                  CASE WHEN( o2.is_cont ) THEN
                      best_scv[7]
                  ELSE
                      NULL
                  END AS split_value,
                  o2.num_dist_value, best_scv[5]
              FROM
              (
                  SELECT s1.tid, s1.nid,
                      {madlib_schema}.__best_scv_aggr
                      (
                          scv,
                          s1.fid,
                          coalesce(s1.split_value,0)
                      ) AS best_scv
                  FROM
                  (
                      SELECT t1.tid, t1.nid, t1.fid, split_value,
                          {madlib_schema}.__scv_aggr
                          (
                              {split_criterion},
                              is_cont, 
                              {num_classes},
                              le,
                              total,
                              t2.node_size
                          ) AS scv
                      FROM
                      (
                          SELECT tid, nid, fid, fval, is_cont,
                              CASE WHEN (is_cont) THEN
                                  fval
                              ELSE
                                  NULL::FLOAT8
                              END AS split_value,
                              CASE WHEN (is_cont) THEN
                                  m4_ifdef(`__GREENPLUM__', `sum(count)', 
                                  `{madlib_schema}.__bigint_array_sum(count)') 
                                  OVER
                                  (
                                      PARTITION BY tid, nid, fid
                                      ORDER BY fval
                                      ROWS BETWEEN UNBOUNDED PRECEDING 
                                          AND CURRENT ROW
                                  )
                              ELSE
                                  count
                              END AS le
                          FROM training_instance_aux
                      ) t1,
                      (
                          {select_stmt}
                      ) t2
                      WHERE t1.tid = t2.tid AND t1.nid = t2.nid AND 
                          t1.fid = t2.fid
                      GROUP BY t1.tid, t1.nid, t1.fid, split_value
                  ) s1
                  GROUP BY s1.tid, s1.nid
              ) o1 
              INNER JOIN {feature_table_name} o2 
              ON o1.best_scv[6]::INT = o2.id
      """.format (
          madlib_schema = madlib_schema,
          output_table = output_table,
          continue_grow = continue_grow,
          confidence_level = confidence_level,
          split_criterion = split_criterion,
          num_classes = num_classes,
          select_stmt = select_stmt,
          feature_table_name = feature_table_name,
      )

    plpy.execute(curstmt)


def __create_tree_tables(madlib_schema, result_tree_table_name):
    """
    @brief For training one decision tree, we need some internal tables
           to store intermediate results. This function creates those
           tables. Moreover, this function also creates the tree table
           specified by user.

    @param result_tree_table_name  The name of the tree specified by user.
    """
    # The table of node_size_aux records the size of each node. It is used
    # for missing value handling.
    plpy.execute("DROP TABLE IF EXISTS node_size_aux CASCADE")
    plpy.execute(
        """
        CREATE TEMP TABLE node_size_aux
        (
            tid             INT,
            nid             INT,
            node_size       INT
        )m4_ifdef(`__GREENPLUM__', `DISTRIBUTED BY (tid,nid)')
        """
    )

    """
    The table below stores the decision tree information just constructed.
    Columns:
          id:             The ID of the node represented by this row. Tree
                          node IDs are unique across all trees. The IDs of
                          all children of a node is made to be continuous.
          tree_location:  An array containing the encoded values of all the
                          features on the path from the root node to the
                          current node. For the root node, the location
                          value is {0}.
          feature:        The ID of the best split feature chosen for the
                          node represented by this row.
          probability:    If forced to make a call for a dominant class
                          at a given point this would be the confidence of the
                          call (this is only an estimated value).
          ebp_coeff:      The total errors used by error based pruning (ebp)
                          based on the specified confidence level. RF does
                          not do EBP therefore for RF nodes, this column always
                          contains 1.
          max_class:      If forced to make a call for a dominant class
                          at a given point this is the selected class.
          scv:            The splitting criteria value (scv) computed at this 
                          node.
          live:           Specifies whether the node should be further split
                          or not. A positive value indicates further split of
                          the node represented by this row is needed.
          num_of_samples: The number of samples at this node.
          parent_id:      Id of the parent branch.
          lmc_nid:        Leftmost child (lmc) node id of the node represented
                          by the current row.
          lmc_fval:       The feature value which leads to the lmc node.
                          An example of getting all the child nodes' ids
                          and condition values
                          1. Get the right most node id
                          SELECT DISTINCT ON(parent_id) id FROM tree_table
                          WHERE parent_id = $pid ORDER BY parent_id, id desc
                          INTO max_child_nid;
                          2. Get child nodes' ids and condition values by a
                             while loop
                          node_count = 1;
                          WHILE (lmc_nid IS NOT NULL) AND
                              (0 < node_count AND lmc_nid <= max_child_nid) LOOP
                              ...
                              lmc_nid  = lmc_nid  + 1;
                              lmc_fval = lmc_fval + 1;
                              SELECT COUNT(id) FROM tree_table
                              WHERE id = $lmc_nid AND parent_id = $pid
                              INTO node_count;
                          END LOOP;
          is_cont:        It specifies whether the selected feature is a
                          continuous feature.
          split_value:    For continuous feature, it specifies the split value.
                          Otherwise, it is of no meaning and fixed to 0.
          tid:            The id of a tree that this node belongs to.
          dp_ids:         An array containing the IDs of the non-continuous
                          features chosen by all ancestors nodes (starting
                          from the root) for splitting.
    
    The table below stores the final decision tree information.
    It is an the table specified by users.
    Please refer the table above for detailed column definition.
    """
    plpy.execute(
        """
        DROP TABLE IF EXISTS {result_tree_table_name} CASCADE;
        """.format(result_tree_table_name = result_tree_table_name)
    )
    plpy.execute(
        """
        CREATE TABLE {result_tree_table_name}
        (
            id              INT,
            tree_location   INT[],
            feature         INT,
            probability     FLOAT,
            ebp_coeff       FLOAT,
            max_class       INTEGER,
            scv             FLOAT,
            live            INT,
            num_of_samples  INT,
            parent_id       INT,
            lmc_nid         INT,
            lmc_fval        INT,
            is_cont         BOOLEAN,
            split_value     FLOAT,
            tid             INT,
            dp_ids          INT[]
        ) m4_ifdef(`__GREENPLUM__', `DISTRIBUTED BY (tid,id)');
        """.format(result_tree_table_name = result_tree_table_name)
    )
    
    """
    The following table stored the auxiliary information for updating the
    association table, so that the updating operation only need to
    join the encoded table with association table once
    """
    plpy.execute("DROP TABLE IF EXISTS assoc_aux CASCADE")
    plpy.execute(
        """
        CREATE TEMP TABLE assoc_aux
        (
            nid         INT,
            fid         INT,
            lmc_id      INT,
            svalue      FLOAT,
            is_cont     BOOL
        ) m4_ifdef(`__GREENPLUM__', `DISTRIBUTED BY (nid)');
        """
    )
    plpy.execute("DROP TABLE IF EXISTS tr_assoc_ping CASCADE")
    plpy.execute("DROP TABLE IF EXISTS tr_assoc_pong CASCADE")
    plpy.execute("DROP TABLE IF EXISTS sf_assoc CASCADE")

m4_changequote(`>>>', `<<<')
m4_ifdef(>>>__GREENPLUM_GE_4_2_1__<<<, >>>
    plpy.execute(
        """
        CREATE TEMP TABLE tr_assoc_ping
        (
            id      BIGINT ENCODING (compresstype=RLE_TYPE),
            nid     INT    ENCODING (compresstype=RLE_TYPE),
            tid     INT    ENCODING (compresstype=RLE_TYPE),
            weight  INT    ENCODING (compresstype=RLE_TYPE)
        )
        WITH(appendonly=true, orientation=column)
        DISTRIBUTED BY(id);
        """
    )

    plpy.execute(
        """
        CREATE TEMP TABLE tr_assoc_pong
        (
            id      BIGINT ENCODING (compresstype=RLE_TYPE),
            nid     INT    ENCODING (compresstype=RLE_TYPE),
            tid     INT    ENCODING (compresstype=RLE_TYPE),
            weight  INT    ENCODING (compresstype=RLE_TYPE)
        )
        WITH(appendonly=true, orientation=column)
        DISTRIBUTED BY(id);
        """
    )

    plpy.execute(
        """
        CREATE TEMP TABLE sf_assoc
        (
            nid     INT    ENCODING (compresstype=RLE_TYPE),
            fid     INT    ENCODING (compresstype=RLE_TYPE)
        )
        WITH(appendonly=true, orientation=column)
        DISTRIBUTED BY(fid);
        """
    )
<<<, >>>
    plpy.execute(
        """
        CREATE TEMP TABLE tr_assoc_ping
        (
            id      BIGINT,
            nid     INT,
            tid     INT,
            weight  INT
        )m4_ifdef(`__GREENPLUM__', `DISTRIBUTED BY (id)');
        """
    )

    plpy.execute(
        """
        CREATE TEMP TABLE tr_assoc_pong
        (
            id      BIGINT,
            nid     INT,
            tid     INT,
            weight  INT
        )m4_ifdef(`__GREENPLUM__', `DISTRIBUTED BY (id)');
        """
    )

    plpy.execute(
        """
        CREATE TEMP TABLE sf_assoc
        (
            nid     INT,
            fid     INT
        )m4_ifdef(`__GREENPLUM__', `DISTRIBUTED BY (fid)');
        """  
    )
<<<)
m4_changequote(>>>`<<<, >>>'<<<)


def __rep_prune_tree(
    madlib_schema       ,
    tree_table_name     ,
    validation_table    ,
    max_num_classes     
):
    """
    @brief Prune the trained tree with "Reduced Error Pruning" algorithm.

    @param tree_table_name   The name of the table containing the tree.
    @param validation_table  The name of the table containing validation set. 
    @param max_num_classes   The count of different classes.
    """
    metatable_name = __get_metatable_name(madlib_schema, tree_table_name)
    id_col_name = preproc.__get_id_column_name(madlib_schema, metatable_name)
    class_col_name = preproc.__get_class_column_name(
                         madlib_schema,
                         metatable_name
                     )

    # the value of class column in validation table must in the KV table
    curstmt = """
              SELECT COUNT(*) AS count
              FROM {validation_table}
              WHERE {madlib_schema}.__to_char({class_col_name}) NOT IN
              (
                  SELECT fval 
                  FROM {class_table_name}
                  WHERE fval IS NOT NULL
              )
              """.format(
                  madlib_schema = madlib_schema,
                  validation_table = validation_table,
                  class_col_name = class_col_name,
                  class_table_name = preproc.__get_classtable_name(
                                         madlib_schema, 
                                         metatable_name
                                     )
              )
    n_t = plpy.execute(curstmt)
    n = util.__get_query_value(n_t, "count")

    util.__assert (
        n == 0,
        'the value of class column in validation table must in training table' 
    )

    table_names = __treemodel_classify_internal(
                      madlib_schema,
                      validation_table,
                      tree_table_name,
                      0
                  )

    encoded_table_name = table_names[0]
    classify_result = table_names[1]
    cf_table_name = classify_result

    # after encoding in classification, class_col_name is fixed to class
    class_col_name = "class"

m4_changequote(`>>>', `<<<')
m4_ifdef(>>>__GREENPLUM_PRE_4_1__<<<, >>>
    plpy.execute("DROP TABLE IF EXISTS tree_rep_pong CASCADE")
    plpy.execute(
        """
        CREATE TEMP TABLE tree_rep_pong AS 
        SELECT * FROM {classify_result}
        LIMIT 0 m4_ifdef(`__GREENPLUM__', `DISTRIBUTED BY (id)')
        """.format(classify_result = classify_result)
    )
<<<)
m4_changequote(>>>`<<<, >>>'<<<)

    while True:
        plpy.execute("DROP TABLE IF EXISTS selected_parent_ids_rep")
        
        plpy.execute(
            """
            CREATE TEMP TABLE selected_parent_ids_rep
            (
                parent_id BIGINT,
                max_class  INT
            ) m4_ifdef(`__GREENPLUM__', `DISTRIBUTED BY (parent_id)')
            """
        )
        
        curstmt = """
                  INSERT INTO selected_parent_ids_rep
                  SELECT parent_id, t.g[1] AS max_class
                  FROM
                  (
                      SELECT parent_id,
                          {madlib_schema}.__rep_aggr_class_count (
                              c.class,
                              s.{class_col_name},
                              {max_num_classes}
                          ) AS g
                      FROM {classify_result} c, {encoded_table_name} s
                      WHERE c.id=s.{id_col_name}
                      GROUP BY parent_id
                  ) t
                  WHERE t.g[2] >= 0 AND
                      t.parent_id IN
                      (
                          SELECT parent_id FROM {tree_table_name}
                          WHERE parent_id NOT IN
                          (
                              SELECT parent_id
                              FROM {tree_table_name}
                              WHERE lmc_nid IS NOT NULL
                          ) and id <> 1
                      )
                  """.format(
                      madlib_schema = madlib_schema,
                      class_col_name = class_col_name,
                      max_num_classes = max_num_classes,
                      classify_result = classify_result,
                      encoded_table_name = encoded_table_name,
                      id_col_name = id_col_name,
                      tree_table_name = tree_table_name
                  )

        plpy.execute(curstmt)

        t = plpy.execute(
                "SELECT parent_id FROM selected_parent_ids_rep limit 1"
            )
        num_parent_ids = util.__get_query_value(t, "parent_id")
        
        if num_parent_ids is None:
           break

m4_changequote(`>>>', `<<<')
m4_ifdef(`__GREENPLUM_PRE_4_1__', >>>
        # for GPDB4.0, update operation can't distribute data across segments
        # we use two tables to update the data
        if classify_result == "tree_rep_pong":
            temp_text = cf_table_name        
        else:
            temp_text = "tree_rep_pong"

        plpy.execute("TRUNCATE " + temp_text)
        
        curstmt = """
                  INSERT INTO {temp_text}(id, class, parent_id, leaf_id)
                  SELECT m.id,  t.max_class, t.parent_id, t.id
                  FROM {classify_result} m, {tree_table_name} t
                  WHERE t.id IN 
                      (
                          SELECT parent_id 
                          FROM selected_parent_ids_rep
                      ) AND m.parent_id = t.id
                  """.format (
                      temp_text = temp_text,
                      classify_result = classify_result,
                      tree_table_name = tree_table_name
                  ) 
        plpy.execute(curstmt)
        
        classify_result = temp_text
<<<, >>>
        curstmt = """
                  UPDATE {classify_result} m set class = t.max_class,
                      parent_id = t.parent_id, leaf_id = t.id
                  FROM {tree_table_name} t
                  WHERE t.id IN 
                  (
                      SELECT parent_id 
                      FROM selected_parent_ids_rep
                  ) AND m.parent_id=t.id
                  """.format(
                      classify_result = classify_result, 
                      tree_table_name = tree_table_name
                  )
        plpy.execute(curstmt)
<<<)
m4_changequote(>>>`<<<, >>>'<<<)
        
        curstmt = """DELETE FROM {tree_table_name} WHERE parent_id IN
                     (SELECT parent_id FROM selected_parent_ids_rep)
                  """.format(tree_table_name = tree_table_name)
        plpy.execute(curstmt)

        curstmt = """UPDATE {tree_table_name} t1 SET lmc_nid = NULL,
                         lmc_fval = NULL, max_class = t2.max_class
                     FROM selected_parent_ids_rep t2
                     WHERE t1.id = t2.parent_id;
                  """.format(tree_table_name = tree_table_name)
        plpy.execute(curstmt)

    plpy.execute(
        """
        DROP TABLE IF EXISTS {encoded_table_name} CASCADE;
        """.format(encoded_table_name = encoded_table_name)
    )


def __ebp_prune_tree(madlib_schema, tree_table_name):
    """
    @brief Prune the trained tree with "Error-based Pruning" algorithm.

    @param tree_table_name  The name of the table containing the tree.
    """
    while True:
        plpy.execute("DROP TABLE IF EXISTS selected_parent_ids_ebp")
        plpy.execute(
            """
            CREATE TEMP TABLE selected_parent_ids_ebp(parent_id BIGINT)
            m4_ifdef(`__GREENPLUM__', `DISTRIBUTED BY(parent_id)')
            """
        )
        
        curstmt = """
                  INSERT INTO selected_parent_ids_ebp
                  SELECT s.parent_id as parent_id
                  FROM
                  (
                      SELECT parent_id, sum(ebp_coeff) AS ebp_coeff
                      FROM
                      (
                          SELECT parent_id, ebp_coeff
                          FROM {tree_table_name}
                          WHERE parent_id NOT IN
                          (
                              SELECT parent_id
                              FROM {tree_table_name}
                              WHERE lmc_nid IS NOT NULL
                          )  AND id <> 1
                      ) m
                         GROUP BY m.parent_id
                  ) s
                  LEFT JOIN  {tree_table_name} p
                  ON p.id = s.parent_id
                  WHERE  p.ebp_coeff < s.ebp_coeff
                  """.format(tree_table_name = tree_table_name)
        plpy.execute(curstmt)
        
        t = plpy.execute(
                """
                SELECT parent_id FROM selected_parent_ids_ebp LIMIT 1
                """
            )
        num_parent_ids = util.__get_query_value(t, "parent_id")

        if num_parent_ids is None:
            break

        curstmt = """
                  DELETE FROM {tree_table_name}
                  WHERE parent_id IN
                  (
                      SELECT parent_id FROM selected_parent_ids_ebp
                  )
                  """.format(tree_table_name = tree_table_name)
        plpy.execute(curstmt)

        curstmt = """
                  UPDATE {tree_table_name}
                  SET lmc_nid = NULL, lmc_fval = NULL
                  WHERE id IN
                  (
                      SELECT parent_id FROM selected_parent_ids_ebp
                  )
                  """.format(tree_table_name = tree_table_name)
        plpy.execute(curstmt)


def __generate_final_tree(madlib_schema, result_tree_table_name):
    """
    @brief Generate the final trained tree.

    @param result_tree_table_name  The name of the table containing the tree.
    """
    plpy.execute(
        """
        DELETE FROM {result_tree_table_name} 
        WHERE COALESCE(num_of_samples, 0) = 0
        """.format(result_tree_table_name = result_tree_table_name)
    )

    # for each node, find the left most child node id and the feature value, 
    # and update the node's lmc_nid and lmc_fval column
    curstmt = """
              UPDATE {result_tree_table_name} k
              SET lmc_nid = g.lmc_nid, lmc_fval = g.lmc_fval
              FROM
              (   
                  SELECT parent_id,
                      min(id) as lmc_nid,
                      min
                      (
                          tree_location[array_upper(tree_location,1)]
                      )
                      AS lmc_fval
                  FROM {result_tree_table_name}
                  GROUP BY parent_id
              ) g
              WHERE k.id = g.parent_id
              """.format(result_tree_table_name = result_tree_table_name)
    plpy.execute(curstmt)

    """
    For a certain node, if all of its children are leaf nodes and have the
    same class label, we can safely remove its children. After removal, we
    should apply the same operation to the new leaf nodes until no nodes
    meet this criterion.
    """
    while True :
        plpy.execute("DROP TABLE IF EXISTS trim_tree_aux_table CASCADE") 
        # Find nodes whose children should be removed.
        curstmt = """
                  CREATE TEMP TABLE trim_tree_aux_table AS
                  SELECT parent_id FROM
                  (
                      SELECT parent_id, count(distinct max_class) AS class_count
                      FROM {result_tree_table_name}
                      WHERE parent_id IN
                      (
                          SELECT parent_id FROM {result_tree_table_name}
                          WHERE parent_id NOT IN
                          (
                              SELECT parent_id 
                              FROM {result_tree_table_name}
                              WHERE lmc_nid IS NOT NULL
                          ) and parent_id <> 0
                      ) 
                      GROUP BY parent_id
                  ) l
                  WHERE l.class_count = 1
                  m4_ifdef(`__GREENPLUM__', `DISTRIBUTED BY (parent_id)')   
                  """.format(result_tree_table_name = result_tree_table_name)
        plpy.execute(curstmt)

        t = plpy.execute("SELECT count(*) AS count FROM trim_tree_aux_table")
        num_redundant_nodes = util.__get_query_value(t, "count")

        if num_redundant_nodes <= 0 :
            break

        # Delete the found redundant nodes.
        curstmt = """
                  DELETE FROM {result_tree_table_name} t   
                  WHERE t.parent_id IN
                  (
                      SELECT parent_id FROM trim_tree_aux_table
                  )
                  """.format(result_tree_table_name = result_tree_table_name)
        plpy.execute(curstmt)

        # Set the nodes, whose children are removed, to be leaf nodes.
        curstmt = """
                  UPDATE {result_tree_table_name} k
                  SET lmc_nid = NULL, lmc_fval = NULL
                  FROM 
                  (
                      SELECT parent_id FROM trim_tree_aux_table
                  ) g
                  WHERE k.id = g.parent_id
                  """.format(result_tree_table_name = result_tree_table_name)
        plpy.execute(curstmt)


def __sample_with_replacement(
    madlib_schema,
    num_of_tree,
    size_per_tree,
    src_table,
    target_table
):
    """
    @brief The function samples with replacement from source table and store
           the results to target table.

           In this function, we firstly calculate how many samples should be
           generated in each segment. Then, we let those segments sample with
           replacement between the maximum ID and minimum ID of the source table
           in parallel and assign samples to different trees.

           If there are gaps in the ID column of the source table, we sample
           extra records in proportion to the number of gaps. At last, we remove
           these invalid samples with an inner join operation with the source
           table. Since we target big data, this strategy works quite well.

    @param num_of_tree     The number of trees to be trained.
    @param size_per_tree   The number of records to be sampled for each tree.
    @param src_table       The name of the table to be sampled from.
    @param target_table    The name of the table used to store the results.
    """
m4_changequote(`>>>', `<<<')
m4_ifdef(>>>__GREENPLUM__<<<, >>>
    # get the segment number
    t = plpy.execute(
            """
            SELECT COUNT(distinct content) AS count 
            FROM gp_segment_configuration
            WHERE content <> -1
            """
        )
    segment_num = util.__get_query_value(t, "count")
<<<, >>>
    # fix the segment number to 1 for PG
    segment_num = 1
<<<)
m4_changequote(>>>`<<<, >>>'<<<)

    plpy.execute("DROP TABLE IF EXISTS auxiliary_segment_table")
    
    plpy.execute(
        """
        CREATE TEMP TABLE auxiliary_segment_table
        (
            segment_id  INT
        ) m4_ifdef(`__GREENPLUM__', `DISTRIBUTED BY(segment_id)')
        """
    )

    # Insert segment_num of records distributed by segment id
    plpy.execute(
        """
        INSERT INTO auxiliary_segment_table 
        SELECT generate_series(1, {segment_num})
        """.format(segment_num = segment_num)
    )

    t = plpy.execute(
            """
            SELECT max(id) as max, min(id) as min, count(id) as record_num
            FROM 
            """ + src_table
        )
    max_id = util.__get_query_value(t, "max")
    min_id = util.__get_query_value(t, "min")
    record_num = util.__get_query_value(t, "record_num")

    range = max_id - min_id + 1

    # compute the sample ratio
    sample_ratio = float(range) / record_num

    # compute how many records should be sampled by each segment
    sample_per_seg = \
        int((sample_ratio * num_of_tree * size_per_tree) / segment_num)
    
    # add the weight field
    if range > record_num :
        # remove those invalid samples with join operation
        stmt = """
               INSERT INTO {target_table}(id, tid, nid, weight)
               SELECT record_id,
                   tid AS tid,
                   tid AS nid,
                   count(*) AS weight
               FROM     
               (
                   SELECT  
                       {madlib_schema}.__sample_within_range
                       (
                           {sample_per_seg}, 
                           {min_id}, {max_id}
                       ) AS record_id,
                       {madlib_schema}.__sample_within_range
                       (
                           {sample_per_seg},
                           1, 
                           {num_of_tree}
                       ) AS tid
                    FROM auxiliary_segment_table
               ) t,
               {src_table} k
               WHERE t.record_id = k.id
               GROUP BY record_id, tid, nid
               """.format (
                   madlib_schema = madlib_schema,
                   target_table = target_table,
                   sample_per_seg = sample_per_seg,
                   min_id = min_id,
                   max_id = max_id,
                   num_of_tree = num_of_tree,
                   src_table = src_table
               )
    else :
        stmt = """
               INSERT INTO {target_table}(id, tid, nid, weight)
               SELECT record_id,
                   tid AS tid,
                   tid AS nid,
                   count(*) AS weight
               FROM
               (
                   SELECT 
                       {madlib_schema}.__sample_within_range
                       (
                           {sample_per_seg}, 
                           {min_id},
                           {max_id}
                       ) AS record_id,
                       {madlib_schema}.__sample_within_range
                       (
                           {sample_per_seg}, 
                           1,
                           {num_of_tree}
                       ) AS tid
                   FROM auxiliary_segment_table
               ) t
               GROUP BY record_id, tid, nid
               """.format (
                   madlib_schema = madlib_schema,
                   target_table = target_table,
                   sample_per_seg = sample_per_seg,
                   min_id = min_id,
                   max_id = max_id,
                   num_of_tree = num_of_tree
               )
    plpy.execute(stmt)


def __train_tree (
    madlib_schema           ,
    split_criterion         ,
    num_trees               ,
    features_per_node       ,
    training_table_name     ,
    training_table_meta     ,
    result_tree_table_name  ,
    validation_table_name   ,
    id_col_name             ,
    class_col_name          ,
    confidence_level        ,
    max_tree_depth          ,
    sampling_percentage     ,
    node_prune_threshold    ,
    node_split_threshold    ,
    sampling_needed         ,
    h2hmv_routine_id        ,
    verbosity               
):
    """
    @brief This function trains a decision tree or random forest.
    @param split_criterion          This parameter specifies which split
                                    criterion should be used for tree 
                                    construction and pruning.
                                    The valid values are infogain, gainratio, 
                                    and gini.
    @param num_trees                Total number of trees to be trained.
    @param features_per_node        Total number of features used to compute 
                                    split gain for each node.
    @param training_table_name      The name of the table/view with the source 
                                    data.
    @param training_table_meta      The name of the table with the meta data.
    @param result_tree_table_na     The name of the table where the resulting
                                    DT/RF will be stored.
    @param validation_table_name    The validation table used for pruning tree.
    @param id_col_name              The name of the column containing id of 
                                    each point.
    @param class_col_name           The name of the column containing correct
                                    class of each point.
    @param confidence_level         A statistical confidence interval of the
                                    resubstitution error.
    @param max_tree_depth           Maximum decision tree depth.
    @param node_prune_threshold     Specifies the minimum number of cases 
                                    required in a child node.
    @param node_split_threshold     Specifies the minimum number of cases 
                                    required in a node in order for a further 
                                    split to be possible.
    @param sampling_needed          Whether enabling the sampling functionality.
    @param max_split_point          The upper limit of sampled split points for
                                    continuous features.
    @param h2hmv_routine_id         Specifies how to handle missing values.
                                    1 ignore, 2 explicit.
    @param verbosity                > 0 means this function runs in verbose 
                                    mode.

    @return The record including training related information.
    @Details please refer to the UDT: MADLIB_SCHEMA.__train_result.
    """
    # declaration
    location = []
    temp_location = []
    sc_code = 1
    curstmt = ""
    grow_tree = max_tree_depth
    class ret : pass               # __train_result
    curr_level = 1
    class instance_time : pass     # __gen_acc_time
    tr_table_index = 1             
    tr_tables = ['tr_assoc_ping', 'tr_assoc_pong']
    cur_tr_table = "tr_assoc_ping"
    need_analyze = True

    # record the time costed in different steps when training
    begin_func_exec     = datetime.datetime.now()
    scv_acs_time        = begin_func_exec - begin_func_exec
    calc_olap_time      = scv_acs_time
    calc_acc_time       = scv_acs_time
    calc_pre_time       = scv_acs_time
    ins_upd_time        = scv_acs_time
    calc_update_best    = scv_acs_time
    calc_update_child   = scv_acs_time
    calc_update_nid     = scv_acs_time
    bld_assoc_time      = scv_acs_time
    prune_time          = scv_acs_time

    if split_criterion == "infogain" :
        sc_code = 1
    elif split_criterion == "gainratio" :
        sc_code = 2
    elif split_criterion == "gini" :
        sc_code = 3
    else :
        plpy.error("Invalid split criterion!")

    num_classes = preproc.__num_of_class(madlib_schema, training_table_meta)

    if verbosity > 0 :
        plpy.info("NUMBER OF CLASSES IN THE TRAINING SET " + str(num_classes))
       
    if num_classes < 2 :
        plpy.error("the number of classes must be greater than 2")

    curstmt = """
              SELECT count(*) AS count
              FROM {training_table_meta}
              WHERE column_type = 'f'
              """.format(training_table_meta = training_table_meta)
    t = plpy.execute(curstmt)
    attr_count = util.__get_query_value(t, "count")

    # generate the horizontal table for updating assinged node IDs
    preproc.__gen_horizontal_encoded_table(
        madlib_schema,
        'tmp_dt_hori_table', 
        training_table_name,
        attr_count, 
        verbosity
    )

    t = plpy.execute("SELECT count(*) AS count FROM tmp_dt_hori_table")
    total_size = util.__get_query_value(t, "count")

    if verbosity > 0 :
        plpy.info("INPUT TABLE SIZE: " + str(total_size))

    begin_bld_assoc = datetime.datetime.now()
    cur_tr_table = tr_tables[tr_table_index - 1]

    """
    The table of tr_assoc holds the information of which records are
    used during training for each tree.
    It has four columns.
        id     --   The id of one record.
        tid    --   The id of a tree.
        nid    --   The id of a node in a tree.
        weight --   The times a record is assigned to a node.
    """
    if sampling_needed :
        plpy.execute(
            """
            SELECT {madlib_schema}.__sample_with_replacement
            (
                {num_trees},
                {sample_size},
                'tmp_dt_hori_table',
                '{cur_tr_table}'
            )
            """.format (
                num_trees = num_trees,
                sample_size = int(round(sampling_percentage * total_size)),
                cur_tr_table = cur_tr_table,
                madlib_schema = madlib_schema
            )
        )
    else :
        curstmt = """
                  INSERT INTO {cur_tr_table}
                  SELECT id, 1 as tid, 1 as nid, 1 as weight
                  FROM tmp_dt_hori_table
                  """.format(cur_tr_table = cur_tr_table)
        plpy.execute(curstmt)

    # analyze ping
    plpy.execute("ANALYZE " + cur_tr_table)
    bld_assoc_time = datetime.datetime.now() - begin_bld_assoc

    # generate the root node for all trees.
    # the generated numbers are the same for the two generate_series
    curstmt = """
              INSERT INTO {result_tree_table_name}
              (
                  id, tree_location, feature, probability, max_class,scv,
                  live, num_of_samples, parent_id, tid
              )
              SELECT generate_series(1, {num_trees}), ARRAY[0], 0, 1, 1, 1, 1,
                  0, 0, generate_series(1, {num_trees})
              """.format(
                  result_tree_table_name = result_tree_table_name, 
                  num_trees = num_trees
              )
    plpy.execute(curstmt)

    max_nid = num_trees
    location_size = 0

    while True :
        t = plpy.execute(
                """
                SELECT COUNT(id) AS count
                FROM {result_tree_table_name}
                WHERE live > 0 AND array_upper(tree_location,1) = {curr_level}
                """.format(
                    result_tree_table_name = result_tree_table_name, 
                    curr_level = curr_level
                )
            )
        num_live_nodes = util.__get_query_value(t, "count")

        if num_live_nodes < 1 :
            if verbosity > 0 :
                plpy.info("EXIT: no live nodes to split")
            break

        if verbosity > 0 :
            plpy.info("Running on level: " + str(curr_level))

        begin_olap_acs = datetime.datetime.now()
 
        instance_time = __gen_acc (
                            madlib_schema,
                            training_table_name,
                            training_table_meta,
                            result_tree_table_name,
                            cur_tr_table,
                            "sf_assoc",
                            features_per_node,
                            num_classes,
                            sampling_needed,
                            verbosity
                        )
        
        if h2hmv_routine_id == 1 :
            # For ignore, we need the true size of nodes to handle the
            # missing values.
            plpy.execute("TRUNCATE node_size_aux")

            curstmt = """
                      INSERT INTO node_size_aux
                      SELECT tr.tid, tr.nid, sum(weight) as count
                      FROM {cur_tr_table} tr
                      GROUP BY tr.tid, tr.nid
                      """.format(cur_tr_table = cur_tr_table)
            plpy.execute(curstmt)

        calc_pre_time  = calc_pre_time + instance_time.calc_pre_time
        calc_acc_time  = calc_acc_time + instance_time.calc_acc_time
        calc_olap_time = calc_olap_time + (datetime.datetime.now() - begin_olap_acs)

        curr_level = curr_level + 1

        begin_find_best = datetime.datetime.now()

        plpy.execute(
            """
            SELECT {madlib_schema}.__find_best_split
            ( 
                'training_instance',
                {confidence_level},
                '{training_table_meta}',
                {sc_code},
                {grow_tree},
                'find_best_answer_table',
                {h2hmv_routine_id},
                {num_classes}
            )
            """.format(
                confidence_level = confidence_level,
                training_table_meta = training_table_meta,
                sc_code = sc_code,
                grow_tree = grow_tree,
                h2hmv_routine_id = h2hmv_routine_id,
                num_classes = num_classes,
                madlib_schema = madlib_schema
            )
        )

        if verbosity > 0 :
            plpy.info(
                "find best time at this level: " + \
                str(datetime.datetime.now() - begin_find_best)
            )

        grow_tree = grow_tree - 1

        scv_acs_time = scv_acs_time + \
                       (datetime.datetime.now() - begin_find_best)
        begin_data_transfer = datetime.datetime.now()
        begin_update_best = datetime.datetime.now()
  
        # We get the calculation result for current level.
        # Update the nodes of previous level firstly.
        curstmt = """
                  UPDATE {result_tree_table_name} t
                  SET feature        = c.feature,
                      probability    = c.probability,
                      max_class      = c.max_class,
                      scv            = c.max_scv,
                      ebp_coeff      = c.ebp_coeff,
                      num_of_samples = c.node_size,
                      live           = 0,
                      is_cont        = c.is_cont,
                      split_value    = c.split_value
                  FROM find_best_answer_table c
                  WHERE t.id = c.node_id AND t.tid = c.tid
                  """.format(result_tree_table_name = result_tree_table_name)
        plpy.execute(curstmt)

        calc_update_best = calc_update_best + \
                           (datetime.datetime.now() - begin_update_best)
        begin_update_child = datetime.datetime.now()

        curstmt = """
                  INSERT INTO {result_tree_table_name}
                  (
                      id, tree_location, feature, probability,
                      max_class, scv, live, parent_id, tid, dp_ids
                  )
                  SELECT {max_nid}+row, array_append(tree_location, fval),
                      0, 1, 1, 1, {curr_level}, ans.node_id, ans.tid,
                      CASE when(NOT ans.is_cont) then
                          array_append( dp_ids, ans.feature)
                      ELSE
                          dp_ids
                      END
                  FROM {result_tree_table_name} tree,
                  (
                      SELECT  *, row_number()
                          OVER (ORDER BY l.tid, l.node_id, l.fval) AS row
                      FROM
                      (
                          SELECT  *,
                              CASE WHEN (is_cont) THEN
                                  generate_series(1,2)
                              ELSE
                                  generate_series(1, distinct_features)
                              END AS fval
                          FROM
                              find_best_answer_table
                          WHERE live > 0 AND 
                              coalesce(feature, 0) <> 0 AND 
                              node_size >= {prune_threshold} AND 
                              node_size >= {split_threshold}
                      ) l
                  ) ans
                  WHERE tree.id=ans.node_id and tree.tid=ans.tid
                  """.format (
                      result_tree_table_name = result_tree_table_name,
                      max_nid = max_nid,
                      curr_level = curr_level,
                      prune_threshold = total_size * node_prune_threshold,
                      split_threshold = total_size * node_split_threshold
                  )

        if verbosity > 0 :
            plpy.info("Generate Child Nodes: " + curstmt)

        plpy.execute(curstmt)

        t = plpy.execute("SELECT max(id) AS max FROM " + result_tree_table_name)
        max_nid = util.__get_query_value(t, "max")

        if verbosity > 0 :
            plpy.info(
                "Max nid:{max_nid}, level:{curr_level}".format(
                    max_nid = max_nid, 
                    curr_level = curr_level
                )
            )

        """
        insert the leftmost child node id and relevant info
        to the assoc_aux table, so that we will make use of this
        info to  update the assigned nid the samples belong to
        the current node whose id is answer.node_id.
        """
        curstmt = """
                  INSERT INTO assoc_aux
                  (
                      nid, fid, lmc_id, svalue, is_cont
                  )
                  SELECT t.id, t.feature, min(l.id),
                      t.split_value, t.is_cont
                  FROM
                  (
                      SELECT id, parent_id
                      FROM {result_tree_table_name}
                      WHERE array_upper(tree_location,1) = {curr_level}
                  ) l,
                  {result_tree_table_name} t
                  WHERE l.parent_id=t.id
                  GROUP BY t.id, t.feature, t.split_value, t.is_cont
                  """.format(
                          result_tree_table_name = result_tree_table_name,
                          curr_level = curr_level
                  )

        if verbosity > 0 :
            plpy.info("Update lmc_child Info: " + curstmt)

        plpy.execute(curstmt)
        
        """
        delete the unused nodes on the previous level
        delete those nodes with a size less than node_prune_threshold
        node_prune_threshold will not apply to root node,
        the level is 1 (curr_level - 1 = 1);
        """
        if curr_level > 2 :
            curstmt = """
                      DELETE FROM {result_tree_table_name} t
                      WHERE t.num_of_samples < {prune_threshold} OR 
                          live = {level}
                      """.format(
                          result_tree_table_name = result_tree_table_name,
                          prune_threshold = total_size * node_prune_threshold,
                          level = curr_level - 1
                      )
            plpy.execute(curstmt)

        calc_update_child   = calc_update_child + \
                              (datetime.datetime.now() - begin_update_child)
        begin_update_nid    = datetime.datetime.now()

        # update the assigned node id for each sample on the current level
        tr_table_index = (tr_table_index % 2) + 1

        curstmt = """
                  INSERT INTO {tr_table} (id, nid, tid, weight)
                  SELECT
                      tr.id,
                      au.lmc_id - 1 +
                      CASE WHEN (au.is_cont) THEN
                          CASE WHEN (svalue < vt.fvals[au.fid]) THEN
                              2
                          ELSE
                              1
                          END
                      ELSE
                          vt.fvals[au.fid]::INT
                      END AS nid,
                          tid, weight
                  FROM {cur_tr_table} tr, tmp_dt_hori_table vt, assoc_aux au
                  WHERE tr.nid = au.nid AND vt.id = tr.id AND 
                      vt.fvals[au.fid] IS NOT NULL
                  """.format(
                      tr_table = tr_tables[tr_table_index - 1],
                      cur_tr_table = cur_tr_table
                  )

        if verbosity > 0 :
            plpy.info(curstmt)

        plpy.execute(curstmt)
        plpy.execute("TRUNCATE " + cur_tr_table)
        cur_tr_table = tr_tables[tr_table_index - 1]

        if need_analyze :
            # analyze pong table
            plpy.execute("ANALYZE " + cur_tr_table)
            need_analyze = False

        plpy.execute("TRUNCATE assoc_aux")

        calc_update_nid = calc_update_nid + \
                          (datetime.datetime.now() - begin_update_nid)

        ins_upd_time = ins_upd_time + \
                       (datetime.datetime.now() - begin_data_transfer)

        if verbosity > 0 :
            plpy.info(
                "computation time in this level: " + \
                str(datetime.datetime.now() - begin_find_best)
            )
    
    __generate_final_tree(madlib_schema, result_tree_table_name)

    begin_prune = datetime.datetime.now()

    
    if confidence_level < 100.0 :
        __ebp_prune_tree(madlib_schema, result_tree_table_name)


    if validation_table_name is not None :
        __rep_prune_tree(
            madlib_schema,
            result_tree_table_name,
            validation_table_name ,
            num_classes
        )
    
    prune_time = datetime.datetime.now() - begin_prune

    if verbosity > 0 :
        plpy.info("time of sampling with replacement: " + str(bld_assoc_time))
        plpy.info(
            "time of finding best and calculating ACS: " + str(scv_acs_time)
        )
        plpy.info("time of calculating ACC: " + str(calc_acc_time))
        plpy.info("time of Insert/update operation: " + str(ins_upd_time))
        plpy.info("time of pruning: " + str(prune_time))
        plpy.info(
            "time of training: " + \
            str(datetime.datetime.now() - begin_func_exec)
        )

    curstmt = """
              SELECT COUNT(id) AS count,
                  max(array_upper(tree_location, 1)) AS max
              FROM {result_tree_table_name}
              """.format(result_tree_table_name = result_tree_table_name)
    t = plpy.execute(curstmt)
    ret.num_tree_nodes = int(util.__get_query_value(t, "count"))
    ret.max_tree_depth = int(util.__get_query_value(t, "max"))

    ret.features_per_node   = int(features_per_node)
    ret.num_of_samples      = int(total_size)
    ret.calc_acc_time       = calc_acc_time
    ret.calc_pre_time       = calc_pre_time
    ret.update_time         = ins_upd_time
    ret.update_best         = calc_update_best
    ret.update_child        = calc_update_child
    ret.update_nid          = calc_update_nid
    ret.scv_acs_time        = scv_acs_time
    ret.prune_time          = prune_time

    return ret


def __display_node_sfunc(
    madlib_schema,
    state       ,
    depth       ,
    is_cont     ,
    feat_name   ,
    curr_val    ,
    split_value ,
    max_prob    ,
    max_class   ,
    num_of_samples   
):
    """
    @brief This is a internal function for displaying one tree node in human
           readable format. It is the step function of aggregation named
           __display_tree_aggr.

    @param state     This variable is used to store the accumulated tree
                     display information.
    @param depth     The depth of this node.
    @param is_cont   Whether the feature used to split is continuous.
    @param feat_name The name of the feature used to split.
    @param curr_val  The value of the splitting feature for this node.
    @param sp_val    For continuous feature, it specifies the split value.
                     Otherwise, it is of no meaning.
    @param max_prob  For those elements in this node, the probability that
                     an element belongs to the max_class.
    @param max_class The class ID with the largest number of elements
                     for those elements in this node.
    @param num_of_samples Total count of elements in this node.

    @return It returns the text containing the information of human
            readable information for trees.
    """
    ret = ""
    for index in range(0, depth + 1) :
        ret += "    "

    if depth > 0 :
        ret += util.__coalesce(feat_name, "null") + ": "
        """
        For continuous features, there are two splits.
        We will mark curr_val to 1 for '<='. Otherwise,
        we will mark curr_val to 2.
        """
        if is_cont :
            if int(curr_val) == 1 :
                ret += " <= "
            else :
                ret += " > "
            ret += str(util.__coalesce(split_value, 0)) + " "
        else :
            ret += " = " + util.__coalesce(curr_val, "null") + " "
    else :
        ret += "Root Node"
    ret += " : class({max_class})    num_elements({num_of_samples})    " + \
           "predict_prob({max_prob})".format(
               max_class = util.__coalesce(max_class, "null"),
               num_of_samples = util.__coalesce(num_of_samples, 0),
               max_prob = util.__coalesce(max_prob, 0)
           )
    ret += "\n"

    # If there exists information, append the information
    # for this node.
    if state is not None :
        ret = state + ret
           
    return ret

    
def __treemodel_display_with_ordered_aggr(
    madlib_schema ,
    tree_table    ,
    tree_id       ,
    max_depth   
):
    """
    @brief Display the trained model with human readable format. This function
           leverages ordered aggregate to display the tree with only one scan of
           the tree_table.

    @param tree_table  The full name of the tree table.
    @param tree_id     The array contains the IDs of the trees to be displayed.
    @param max_depth   The max depth to be displayed. If it is set to null,
                       this function will show all levels.

    @return The text representing the tree with human readable format.
    """
    util.__assert_table(madlib_schema, tree_table, True)

    metatable_name = __get_metatable_name(madlib_schema, tree_table)

    """
    This table is used for tree display.
    It is filled with the original information before
    encoding to facilitate the display procedure.
    """
    plpy.execute("DROP TABLE IF EXISTS auxiliary_tree_display")
    plpy.execute(
        """
        CREATE TEMP TABLE auxiliary_tree_display
        (
            tid                     INT,
            id                      INT,
            tree_location           INT[],
            probability             FLOAT8,
            max_class               TEXT,
            num_of_samples          INT,
            parent_id               INT,
            curr_value              TEXT,
            parent_feature_id       INT,
            is_parent_feature_cont  BOOLEAN,
            parent_split_value      FLOAT8,
            parent_feature_name     TEXT
        ) m4_ifdef(`__GREENPLUM__', `DISTRIBUTED BY (id)');
        """
    )

    # We made a self join for the tree table. For each node, we get the
    # feature information at its parent node so as to display this node.
    curstmt = """
              INSERT INTO auxiliary_tree_display
              SELECT m.*,
                  n.column_name as parent_feature_name
              FROM
              (
                  SELECT * FROM
                  (
                      SELECT t1.tid,t1.id, t1.tree_location,
                          t1.probability, t1.max_class::TEXT,
                          t1.num_of_samples, t1.parent_id,
                          t1.tree_location[array_upper(t1.tree_location,1)]
                          ::TEXT AS curr_value,
                          t2.feature as parent_feature_id,
                          t2.is_cont as is_parent_feature_cont,
                          t2.split_value as parent_split_value
                      FROM {tree_table} t1 LEFT JOIN {tree_table} t2 ON
                      (
                          t1.parent_id = t2.id AND
                          (coalesce(t1.tid,0)=coalesce(t2.tid,0)) 
                      ) 
                  ) l
                  WHERE l.tid in ({tree_id}) 
              ) m
              LEFT JOIN {metatable_name} n
              ON m.parent_feature_id = n.id;
              """.format ( 
                  tree_table = tree_table,
                  tree_id = ",".join(str(id) for id in tree_id),
                  metatable_name = metatable_name
              )
    plpy.execute(curstmt)

    # Get the metatable storing the encoding information of class.
    curstmt = """
              SELECT column_name,
                  {madlib_schema}.__regclass_to_text(table_oid) AS table_name
              FROM  {metatable_name}
              WHERE column_type = 'c' LIMIT 1
              """.format(
                  metatable_name = metatable_name,
                  madlib_schema = madlib_schema
              )
    t = plpy.execute(curstmt)

    table_name = util.__get_query_value(t, "table_name")

    if table_name is not None :
        # Convert back for the class column.
        curstmt = """
                  UPDATE auxiliary_tree_display n
                  SET max_class = {madlib_schema}.__to_char(m.fval)
                  FROM {table_name} m
                  WHERE m.code = n.max_class::INT
                  """.format (
                      madlib_schema = madlib_schema,
                      table_name = table_name
                  )
        plpy.execute(curstmt)

    # Get the metatables storing the encoding information for discrete features.
    curstmt = """
              SELECT
                  id,
                  column_name,
                  {madlib_schema}.__regclass_to_text(table_oid) as table_name
              FROM {metatable_name}
              WHERE NOT is_cont AND column_type='f';
              """.format(
                  metatable_name = metatable_name, 
                  madlib_schema = madlib_schema
              )
    result_rec_t = plpy.execute(curstmt)

    for result_rec in result_rec_t :
        feature_name = result_rec["column_name"]
        table_name = result_rec["table_name"]
        id = result_rec["id"]
        curstmt = """
                  UPDATE auxiliary_tree_display n
                  SET curr_value = {madlib_schema}.__to_char(m.fval)
                  FROM {table_name} m
                  WHERE m.code::INT = n.curr_value::INT
                      AND m.fid = {id}
                      AND n.parent_feature_name = '{feature_name}'
                  """.format (
                      feature_name = feature_name,
                      table_name = table_name,
                      id = id,
                      madlib_schema = madlib_schema
                  )
        plpy.execute(curstmt)

    """
    Now we already get all the information. Invoke the
    aggregation to show the tree.
    If we order by tree_location, we can get the sequence
    of depth first traversal.
    """
    curstmt = """
              SELECT tid, 
                  {madlib_schema}.__display_tree_aggr
                  (
                      array_upper(tree_location,1)-1,
                      is_parent_feature_cont,
                      parent_feature_name,
                      curr_value,
                      parent_split_value,
                      probability,
                      max_class,
                      num_of_samples
                      order by tree_location
                  ) AS disp_str
              FROM auxiliary_tree_display
              """.format(madlib_schema = madlib_schema)
    if max_depth is not None :
        curstmt += "WHERE array_upper(tree_location,1) - 1 <=" + str(max_depth)
    
    curstmt += "GROUP BY tid ORDER BY tid;"
    result_rec_t = plpy.execute(curstmt)
    
    results = []
    for result_rec in result_rec_t :
        result = "\nTree {tid}\n{disp_str}".format(
                     tid = str(result_rec["tid"]), 
                     disp_str = result_rec["disp_str"]
                 )
        results.append(result)

    return results


def __display_tree_no_ordered_aggr (
    madlib_schema   ,
    tree_table      ,
    id              ,
    feature_id      ,
    depth           ,
    is_cont         ,
    split_value     ,
    metatable_name  ,
    max_depth       ,
    tree_id         
):
    """
    @brief This is an internal function for displaying the tree in human 
           readable format.It use the depth-first strategy to traverse a 
           tree and print
           values.

    @param tree_table      The full name of the tree table.
    @param id              The id of current node. This node and all of 
                           its children are displayed.
    @param feature_id      The ID of a feature, which was used to split 
                           in the parent of current node.
    @param depth           The depth of current node.
    @param is_cont         It specifies whether the feature denoted by 
                           'feature_id' is continuous or not.
    @param split_value     For continuous feature, it specifies the split 
                           value.
                           Otherwise, it is of no meaning.
    @param metatable_name  For tabular format, this table contains the meta 
                           data to encode the input table.
    @param max_depth       The max depth to be displayed. If it is set to 
                           null, this function will show all levels.
    @param tree_id         The ID of the tree to be displayed.

    @return The text representing the tree with human readable format.
    """
    ret = ""
    if (id is None) or (id <= 0) :
        return ret

    curstmt = """
              SELECT tree_location, feature, is_cont,
                  split_value, max_class,num_of_samples,probability
              FROM {tree_table}
              WHERE id = {id} AND tid = {tree_id}
              """.format (
                  tree_table = tree_table,
                  id = str(id),
                  tree_id = str(tree_id)
              )  
    t = plpy.execute(curstmt)

    tree_location    = util.__get_query_value(t,"tree_location")
    feature          = util.__get_query_value(t,"feature")
    is_cont          = util.__get_query_value(t,"is_cont")
    temp_split_value = util.__get_query_value(t,"temp_split_value")
    max_class        = util.__get_query_value(t,"max_class")
    num_of_samples   = util.__get_query_value(t,"num_of_samples")
    probability      = util.__get_query_value(t,"probability") 

    curr_value = tree_location[len(tree_location) - 1]
    
    for index in range(0, depth) :
        ret += "    "

    if id > tree_id :
        ret += preproc.__get_feature_name(
                   madlib_schema, 
                   feature_id, 
                   metatable_name
               ) + ": "

        if is_cont :
            if curr_value == 1 :
                ret += " <= "
            else :
                ret += " > "
            ret += str(split_value)
	else :
	    ret += " = " + preproc.__get_feature_value(
                           madlib_schema,
                           feature_id,
                           curr_value, 
                           metatable_name
                       ) 
    else :
        ret += "Root Node "

    ret += " : class({class_value})   num_elements({num_elements})  " + \
           "predict_prob({predict_prob})".format (
               class_value = preproc.__get_class_value(
                                 madlib_schema, 
                                 max_class,
                                 metatable_name
                             ),
               num_elements = str(num_of_samples),
               predict_prob = str(probability)
           )

    ret += "\n"

    if (max_depth is not None) and (depth >= max_depth) :
        return ret

    curstmt = """
              SELECT id
              FROM {tree_table}
              WHERE parent_id = {id} AND tid = {tree_id}
              ORDER BY id
              """.format (
                  tree_table = tree_table,
                  id = str(id),
                  tree_id = str(tree_id)
              )

    child_nid_t = plpy.execute(curstmt)

    for child_nid in child_nid_t :
        ret += __display_tree_no_ordered_aggr (
                   madlib_schema,
                   tree_table,
                   child_nid["id"],
                   feature,
                   depth + 1,
                   is_cont,
                   temp_split_value,
                   metatable_name,
                   max_depth,
                   tree_id
               )

    return ret


def __treemodel_get_vote_result(madlib_schema, src_table, dst_table):
    """
    @brief Multiple trees may classify the same record to different classes.
           This function gets the results voted by multiple trees.

    @param src_table    The full name of the table containing original data.
    @param dst_table    The full name of the table to store the voted results.
    """
    plpy.execute("DROP TABLE IF EXISTS " + dst_table)
    plpy.execute(
        """
        CREATE TEMP TABLE {dst_table}
        (
            id          INT,
            class       INT,
            prob        FLOAT8
        )m4_ifdef(`__GREENPLUM__', `DISTRIBUTED BY (id)')
        """.format(dst_table = dst_table) 
    )
  
    curstmt = """
              INSERT INTO {dst_table}
              SELECT id, max_array[3], max_array[2] 
              FROM
              (
                  SELECT id, max(array[count,prob,class]) AS max_array 
                  FROM
                  (
                      SELECT id, class, COUNT(*) AS count, AVG(prob) as prob
                      FROM {src_table}
                      GROUP BY id,class
                  ) t1 
                  GROUP BY id
              ) t2
              """.format(
                  dst_table = dst_table,
                  src_table = src_table
              )

    plpy.execute(curstmt)


def __treemodel_classify_internal(
    madlib_schema,
    classification_table_name,
    tree_table_name,
    verbosity
):
    """
    @brief An internal classification function. It classifies with all trees
           at the same time. For medium/small data sets, tests shows that it 
           is more efficient than the serial classification function.

    @param classification_table_name  The full name of the table containing 
                                      the classification set.
    @param tree_table_name            The full name of the tree table.
    @param verbosity                  > 0 means this function runs in verbose 
                                      mode.

    @return An array containing the encoded table name and classification
            result table name (We encode the source table during the 
            classification).
    """
    table_pick         = 1
    id_col_name        = "id"
    curr_level         = 1
    max_level          = 0
    h2hmv_routine_id   = 0
    result_table_name  = "dt_classify_internal_rt"
    encoded_table_name = "dt_classify_internal_edt"
    table_names = ["classified_instance_ping", "classified_instance_pong"]
    metatable_name     = ""    

    time_stamp = datetime.datetime.now()

    util.__assert(
        classification_table_name is not None and
        util.__table_exists(madlib_schema, classification_table_name),
        "the specified classification table <" + 
        classification_table_name + "> does not exists"
    )

    util.__assert(
        tree_table_name is not None and 
        util.__table_exists(madlib_schema, tree_table_name),
        "the specified tree table <" + tree_table_name + \
        "> does not exists"
    )

    util.__assert(verbosity is not None, "verbosity must be non-null") 

    plpy.execute(
        """
        DROP TABLE IF EXISTS  {encoded_table_name} CASCADE
        """.format(encoded_table_name = encoded_table_name)
    )

    metatable_name = __get_metatable_name(madlib_schema, tree_table_name)

    h2hmv_routine_id = __get_routine_id(madlib_schema, tree_table_name)

    preproc.__encode_table_classify (
        madlib_schema,
        classification_table_name,
        encoded_table_name,
        metatable_name,
        h2hmv_routine_id,
        verbosity
    )

    if verbosity > 0 :
        plpy.info("abular format. id_col_name: " + id_col_name)

    """
    The table of classified_instance_ping and classified_instance_pong are
    auxiliary tables used during the classification process.
    For each record, these tables tell us which node it belongs to. They also
    hold the information of class and probability.
    We use transfer data between these two tables rather than update a single
    table during the classification process. We find the operation of update
    is quite expensive.
    """
    plpy.execute("DROP TABLE IF EXISTS classified_instance_ping")

    plpy.execute(
        """
        CREATE TEMP TABLE classified_instance_ping
        (
            tid         INT,
            id          BIGINT,
            jump        INT,
            class       INT,
            prob        FLOAT,
            parent_id   INT,
            leaf_id     INT
        ) m4_ifdef(`__GREENPLUM__', `DISTRIBUTED BY (id)');
        """
    )

    plpy.execute("DROP TABLE IF EXISTS classified_instance_pong")

    plpy.execute(
        """
        CREATE TEMP TABLE classified_instance_pong
        (
            tid         INT,
            id          BIGINT,
            jump        INT,
            class       INT,
            prob        FLOAT,
            parent_id   INT,
            leaf_id     INT
        ) m4_ifdef(`__GREENPLUM__', `DISTRIBUTED BY (id)')
        """
    )

    plpy.execute("DROP TABLE IF EXISTS " + result_table_name + " CASCADE")

    plpy.execute(
        """
        CREATE TABLE {result_table_name}
        (
            tid         INT,
            id          BIGINT,
            jump        INT,
            class       INT,
            prob        FLOAT,
            parent_id   INT,
            leaf_id     INT
        ) m4_ifdef(`__GREENPLUM__', `DISTRIBUTED BY (id)')
        """.format(result_table_name = result_table_name)
    )

    stmt = """
           INSERT INTO classified_instance_ping (id, jump, class, prob,tid)
           SELECT m.{id_col_name}, t.id, 0, 0, t.tid
           FROM {encoded_table_name} m CROSS JOIN
           (
               SELECT DISTINCT tid,id 
               FROM {tree_table_name} 
               WHERE parent_id = 0
           ) t
           """.format (
               id_col_name = id_col_name,
               encoded_table_name = encoded_table_name,
               tree_table_name = tree_table_name
           )
    plpy.execute(stmt)

    t = plpy.execute(
            "SELECT max(array_upper(tree_location,1)) AS max FROM " + \
            tree_table_name
        )
    max_level = util.__get_query_value(t, "max")

    if max_level is None :
        plpy.error("tree should not be empty")

    for curr_level in xrange(1, max_level + 1):
        if verbosity > 0 :
            plpy.info("new_depth: " + str(curr_level))

        table_pick = table_pick % 2 + 1

        plpy.execute("TRUNCATE " + str(table_names[table_pick - 1]))
        
        t = plpy.execute(
                "SELECT count(id) as count FROM " + result_table_name
            )
        size_finished = util.__get_query_value(t, "count")

        if verbosity > 0 :
            plpy.info("size_finished " + str(size_finished))

        t = plpy.execute(
                "SELECT count(*) AS count FROM " + \
                table_names[(table_pick) % 2 + 1 - 1]
            )
        remains_to_classify = util.__get_query_value(t, "count")

        if remains_to_classify == 0 :
            if verbosity > 0 :
                plpy.info(
                    "size_finished: {size_finished} remains_to_classify: " + \
                    "{remains_to_classify}".format(
                        size_finished = size_finished,
                        remains_to_classify = remains_to_classify
                    )         
                ) 

            break

        curstmt = """
                  INSERT INTO {table1}
                  SELECT pt.tid, pt.id,
                  CASE WHEN (is_cont) THEN
                      CASE WHEN (gt.lmc_nid IS NULL) THEN
                          0
                      ELSE
                          gt.lmc_nid + 
                          float8lt(gt.split_value, fvals[gt.feature])::INT4 + 
                          1 - gt.lmc_fval
                      END
                  ELSE
                      CASE WHEN (gt.lmc_nid IS NULL) THEN
                          0
                      ELSE
                          gt.lmc_nid + fvals[gt.feature] - gt.lmc_fval
                      END
                  END as newjump, 
                      gt.max_class, gt.probability, gt.parent_id, gt.id
                  FROM
                  (
                      SELECT t1.tid, t1.id, t1.jump, fvals
                      FROM {table2} t1
                      LEFT JOIN {encoded_table_name} t2
                      ON t1.id = t2.id
                  )  AS pt,
                  (
                      SELECT tid,lmc_nid, lmc_fval, max_class,feature, 
                          probability, parent_id, id, is_cont, split_value
                      FROM {tree_table_name}
                      WHERE array_upper(tree_location,1) = {curr_level}
                  ) AS gt
                  WHERE pt.jump = gt.id AND pt.tid=gt.tid
                  """.format (
                      table1 = table_names[table_pick - 1],
                      table2 = table_names[(table_pick) % 2 + 1 -1],
                      encoded_table_name = encoded_table_name,
                      tree_table_name = tree_table_name,
                      curr_level = curr_level
                  )
                    
        plpy.execute(curstmt)
        
        """
        if the node (whose id is "jump") doesn't exist,
        then insert them into result table
        (be classified to max_class of its corrsponding node)
        """
        ts = plpy.execute("SELECT DISTINCT tid FROM " + tree_table_name)
        for ts_r in ts :
            tree_id = ts_r.get("tid")

            curstmt = """
                      INSERT INTO {result_table_name}
                      (
                          tid,id, jump, class, prob, parent_id, leaf_id
                      )
                      SELECT tid,id, 0, class, prob, parent_id, leaf_id
                      FROM {table_name}
                      WHERE jump NOT IN 
                      (
                          SELECT id 
                          FROM {tree_table_name} 
                          WHERE tid={tree_id}
                      )
                      AND tid={tree_id}
                      """.format(
                          result_table_name = result_table_name,
                          table_name = table_names[table_pick - 1],
                          tree_table_name = tree_table_name,
                          tree_id = tree_id
                      )
            plpy.execute(curstmt)

            # delete from the being classified data table
            curstmt = """
                      DELETE FROM {table_name}
                      WHERE jump NOT IN 
                      (
                          SELECT id 
                          FROM {tree_table_name} 
                          WHERE tid={tree_id}
                      )
                      AND tid={tree_id}
                      """.format(
                          table_name = table_names[table_pick - 1],
                          tree_table_name = tree_table_name,
                          tree_id = tree_id
                      )
            plpy.execute(curstmt)

    plpy.execute(
        """
        INSERT INTO {result_table_name} 
        SELECT * 
        FROM {table_name} 
        WHERE jump = 0
        """.format(
            result_table_name = result_table_name,
            table_name = table_names[table_pick - 1]
        )
    )
    
    plpy.execute(
        """
        INSERT INTO {result_table_name}  
        SELECT *  
        FROM {table_name}   
        WHERE jump = 0
        """.format(
            result_table_name = result_table_name,
            table_name = table_names[table_pick % 2 + 1 - 1]
        )
    )
    
    if verbosity > 0 :
        plpy.info(
            "final classification time:" + \
            str(datetime.datetime.now() - time_stamp)
        )

    return [encoded_table_name, result_table_name]


def __treemodel_classify_internal_serial (
    madlib_schema               ,
    classification_table_name   ,
    tree_table_name             ,
    verbosity                   
):
    """
    @brief An internal classification function. It classifies with one tree
           after another. For large data sets, tests shows that it is more
           efficient than the parallel classification function.

    @param classification_table_name  The full name of the table containing the
                                      classification set.
    @param tree_table_name            The full name of the tree table.
    @param verbosity                  > 0 means this function runs in verbose 
                                      mode.

    @return An array containing the encoded table name and classification result
            table name (We encode the source table during the classification).
    """
    table_pick = 1
    id_col_name = "id"
    curr_level = 1
    max_level = 0
    h2hmv_routine_id = 0
    result_table_name = "dt_classify_internal_rt"
    encoded_table_name = "dt_classify_internal_edt"
    table_names = ['classified_instance_ping', 'classified_instance_pong']

    time_stamp = datetime.datetime.now()

    util.__assert (
        (classification_table_name is not None) and 
        util.__table_exists(madlib_schema, classification_table_name),
        "the specified classification table <" + classification_table_name + \
        "> does not exists"
    )
 
    util.__assert (
        (tree_table_name is not None) and 
        util.__table_exists(madlib_schema, tree_table_name),
        "the specified tree table <" + tree_table_name + "> does not exists"
    )

    util.__assert (
        verbosity is not None,
        "verbosity must be non-null"
    )

    plpy.execute("DROP TABLE IF EXISTS " + encoded_table_name + " CASCADE")

    metatable_name = __get_metatable_name(madlib_schema, tree_table_name)

    h2hmv_routine_id = __get_routine_id(madlib_schema, tree_table_name)

    preproc.__encode_table_classify (
        madlib_schema,
        classification_table_name,
        encoded_table_name,
        metatable_name,
        h2hmv_routine_id,
        verbosity
    )

    if verbosity > 0 :
        plpy.info("tabular format. id_col_name: " + str(id_col_name))
    """
    The table of classified_instance_ping and classified_instance_pong are
    auxiliary tables used during the classification process.
    For each record, these tables tell us which node it belongs to. They also
    hold the information of class and probability.
    We use transfer data between these two tables rather than update a single
    table during the classification process. We find the operation of update
    is quite expensive.
    """
    plpy.execute("DROP TABLE IF EXISTS classified_instance_ping")
    
    plpy.execute(
        """
        CREATE TEMP TABLE classified_instance_ping
        (
            id          BIGINT,
            jump        INT,
            class       INT,
            prob        FLOAT,
            parent_id   INT,
            leaf_id     INT
        ) m4_ifdef(`__GREENPLUM__', `DISTRIBUTED BY (id)')
        """
    )

    plpy.execute("DROP TABLE IF EXISTS classified_instance_pong")

    plpy.execute(
        """
        CREATE TEMP TABLE classified_instance_pong
        (
            id          BIGINT,
            jump        INT,
            class       INT,
            prob        FLOAT,
            parent_id   INT,
            leaf_id     INT
        ) m4_ifdef(`__GREENPLUM__', `DISTRIBUTED BY (id)')
        """
    )

    plpy.execute("DROP TABLE IF EXISTS " + result_table_name + " CASCADE")

    plpy.execute(
        """
        CREATE TEMP TABLE {result_table_name}
        (
            tid         INT,
            id          BIGINT,
            jump        INT,
            class       INT,
            prob        FLOAT,
            parent_id   INT,
            leaf_id     INT
        ) m4_ifdef(`__GREENPLUM__', `DISTRIBUTED BY (id)')
        """.format(result_table_name = result_table_name)
    )

    t = plpy.execute("SELECT DISTINCT tid FROM " + tree_table_name)
    
    for r in t :
        tree_id = r["tid"]
        t = plpy.execute(
                """
                SELECT max(array_upper(tree_location,1)) AS max
                FROM {tree_table_name}
                where tid = {tree_id}
                """.format(
                    tree_table_name = tree_table_name, 
                    tree_id = tree_id
                )
            )
        max_level = util.__get_query_value(t, "max")
     
        if max_level is None :
            plpy.error("tree should not be empty")

        plpy.execute("TRUNCATE classified_instance_ping")
        plpy.execute("TRUNCATE classified_instance_pong")

        t = plpy.execute(
                """
                SELECT id FROM {tree_table_name}
                WHERE parent_id = 0 AND tid={tid}
                """.format(
                    tree_table_name = tree_table_name,
                    tid = tree_id
                )
            )
        root_id = util.__get_query_value(t, "id")

        curstmt = """
                  INSERT INTO classified_instance_ping (id, jump, class, prob)
                  SELECT {id_col_name}, {root_id}, 0, 0 
                  FROM {encoded_table_name}
                  """.format (
                      id_col_name = id_col_name,
                      root_id = root_id,
                      encoded_table_name = encoded_table_name
                  )
        plpy.execute(curstmt)

        table_pick = 1
        for curr_level in range(1, max_level + 1) :
            if verbosity > 0 :
                plpy.info("new_depth: " + str(curr_level))

            table_pick = table_pick % 2 + 1

            plpy.execute("TRUNCATE " + str(table_names[table_pick - 1]))

            t = plpy.execute(
                "SELECT count(id) AS count FROM " + result_table_name
                )
            size_finished = util.__get_query_value(t, "count")

            if verbosity > 0 :
                plpy.info("size_finished  " + str(size_finished))

            # table_names[(table_pick) % 2 + 1 - 1], as ARRAY in pgsql starts
            # index 1, and list in python starts index 0
            t = plpy.execute(
                    "SELECT count(*) AS count FROM " + \
                    table_names[(table_pick) % 2 + 1 - 1]
                )
            remains_to_classify = util.__get_query_value(t, "count")

            if remains_to_classify == 0 :
                if verbosity > 0 :
                    plpy.info(
                        "size_finished: " + str(size_finished) + \
                        " remains_to_classify: " + \
                        str(remains_to_classify)
                    )
            
                break
            
            curstmt = """
                      INSERT INTO {table1}
                      SELECT pt.id,
                          CASE WHEN (is_cont) THEN
                              CASE WHEN (gt.lmc_nid IS NULL) THEN
                                  0
                              ELSE
                                  gt.lmc_nid +
                                  float8lt
                                  (
                                      gt.split_value, fvals[gt.feature]
                                  )::INT4
                                  + 1 - gt.lmc_fval
                              END
                          ELSE
                              CASE WHEN (gt.lmc_nid IS NULL) THEN
                                  0
                              ELSE
                                  gt.lmc_nid + fvals[gt.feature] - gt.lmc_fval
                              END
                          END as newjump,
                          gt.max_class, gt.probability, gt.parent_id, gt.id
                      FROM
                      (
                          SELECT t1.id, t1.jump, fvals
                          FROM {table2} t1
                          LEFT JOIN {encoded_table_name} t2
                          ON t1.id = t2.id
                      ) AS pt,
                      ( 
                          SELECT  lmc_nid, lmc_fval, max_class, feature, 
                              probability,
                          parent_id, id, is_cont, split_value
                          FROM {tree_table_name}
                          WHERE array_upper(tree_location, 1) = {curr_level} 
                              AND tid = {tree_id}
                      ) AS gt
                      WHERE pt.jump = gt.id
                      """.format (
                          table1 = table_names[table_pick - 1],
                          table2 = table_names[(table_pick) % 2 + 1 - 1],
                          encoded_table_name = encoded_table_name,
                          tree_table_name = tree_table_name,
                          curr_level = curr_level,
                          tree_id = tree_id
                      )
            plpy.execute(curstmt)

            """
            if the node (whose id is "jump") doesn't exist,
            then insert them into result table
            (be classified to max_class of its corrsponding node)
            """
            curstmt = """
                      INSERT INTO {result_table_name}
                      (
                          tid,id, jump, class, prob, parent_id, leaf_id
                      )
                      SELECT {tree_id},id, 0, class, prob, parent_id, leaf_id
                      FROM {table_pick_name}
                      WHERE jump NOT IN 
                      (
                          SELECT id FROM {tree_table_name} WHERE tid = {tree_id}
                      )
                      """.format (
                          result_table_name = result_table_name,
                          table_pick_name = table_names[table_pick - 1],
                          tree_table_name = tree_table_name,
                          tree_id = tree_id
                      )
            plpy.execute(curstmt)

            # delete from the being classified data table
            curstmt = """
                      DELETE FROM {table_pick_name}
                      WHERE jump NOT IN 
                      (
                          SELECT id FROM {tree_table_name} WHERE tid = {tree_id}
                      )
                      """.format (
                          table_pick_name = table_names[table_pick - 1],
                          tree_table_name = tree_table_name,
                          tree_id = tree_id   
                      )
            plpy.execute(curstmt)

        curstmt = """
                  INSERT INTO {result_table_name} 
                  SELECT {tree_id}, * 
                  FROM {table_pick_name} WHERE jump = 0
                  """.format (
                      result_table_name = result_table_name,
                      tree_id = tree_id,
                      table_pick_name = table_names[table_pick - 1] 
                  )
        plpy.execute(curstmt)

        curstmt = """
                  INSERT INTO {result_table_name} 
                  SELECT {tree_id}, * 
                  FROM {table_pick_name} WHERE jump = 0
                  """.format (
                      result_table_name = result_table_name,
                      tree_id = tree_id,
                      table_pick_name = table_names[table_pick % 2 + 1 - 1]
                  )
        plpy.execute(curstmt)

    if verbosity > 0 :
        plpy.info(
            "final classification time: " + \
            str(datetime.datetime.now() - time_stamp)
        )

    return [encoded_table_name, result_table_name]


def __treemodel_score (
    madlib_schema               ,
    tree_table_name             ,
    scoring_table_name          ,
    verbosity                   
):
    """
    @brief This function check the accuracy of the trained tree model.

    @param tree_table_name     The name of the tree containing the model.
    @param scoring_table_name  The full name of the table/view with the
                               data to be scored.
    @param verbosity           > 0 means this function runs in verbose mode.

    @return The estimated accuracy information.
    """
    id_col_name = "id"
    class_col_name = "class"
    num_of_row = 0.0
    mis_of_row = 0.0
    table_names = []
    encoded_table_name = ""    

    if verbosity > 0 :
        # get rid of the messages whose severity level is lower than 'WARNING'
        plpy.execute("SET client_min_messages = WARNING")

    util.__assert(
        tree_table_name is not None and 
        util.__table_exists(madlib_schema, tree_table_name),
        "the specified tree table <" + \
        tree_table_name              + \
        "> does not exist"
    )

    util.__assert(
        scoring_table_name is not None and
        util.__table_exists(madlib_schema, scoring_table_name),
        "the specified scoring table <" + \
        str(scoring_table_name)         + \
        "> does not exist"
    )

    util.__assert(
        util.__column_exists(
            madlib_schema, scoring_table_name, 
            preproc.__get_class_column_name(
                madlib_schema, __get_metatable_name( 
                    madlib_schema, tree_table_name
                )
            )
        ),
        "the specified scoring table <" + scoring_table_name + \
        "> does not have class column"
    )

    table_names = __treemodel_classify_internal(
                      madlib_schema,
                      scoring_table_name, 
                      tree_table_name,
                      verbosity
                  )

    encoded_table_name = table_names[0]
    result_table_name = table_names[1]
    result_table_name_final = result_table_name + "_final"

    __treemodel_get_vote_result(
        madlib_schema,
        result_table_name, 
        result_table_name_final
    )

    curstmt = "SELECT count(id) AS count FROM " + result_table_name_final    
 
    t = plpy.execute(curstmt)
    num_of_row = util.__get_query_value(t, "count")

    curstmt = """
              SELECT count(t2.id) AS count
              FROM {encoded_table_name} t1, {result_table_name_final} t2
              WHERE t1.{id_col_name} = t2.id AND t1.{class_col_name} <> t2.class
              """.format (
                  encoded_table_name = encoded_table_name,
                  result_table_name_final = result_table_name_final, 
                  id_col_name = id_col_name,
                  class_col_name = class_col_name
              )

    t = plpy.execute(curstmt)
    mis_of_row = util.__get_query_value(t, "count")

    plpy.execute("DROP TABLE IF EXISTS " + encoded_table_name)
    plpy.execute("DROP TABLE IF EXISTS " + result_table_name)
    plpy.execute("DROP TABLE IF EXISTS " + result_table_name_final)

    return float(num_of_row - mis_of_row) / num_of_row


def __treemodel_clean (
    madlib_schema,
    model_table_name
):
    """
    @brief Cleanup the trained model table and any relevant tables.

    @param model_table_name The name of the table containing
                            the model's information.

    @return The status of that cleanup operation.
    """
    # get rid of the messages whose severity level is lower than 'WARNING'
    plpy.execute("SET client_min_messages = WARNING")

    t = plpy.execute(
            """
            SELECT {madlib_schema}.__table_exists('{model_table_name}') 
                AS existence
            """.format(
                madlib_schema = madlib_schema,
                model_table_name = model_table_name
            )
        )
    util.__assert (
        model_table_name is not None and 
        util.__get_query_value(t, "existence"),
        "the specified tree table <" + model_table_name + "> does not exists"
    )   

    t = plpy.execute(
            """
            SELECT {madlib_schema}.__table_exists
            (
                '{madlib_schema}.training_info'
            ) AS existence
            """.format(madlib_schema = madlib_schema)
        )
    table_exists = util.__get_query_value(t, "existence")
    
    if table_exists :
        metatable_name = __get_metatable_name(madlib_schema, model_table_name)

        if metatable_name is not None :
            t = plpy.execute(
                    """
                    SELECT count(*) AS count
                    FROM {madlib_schema}.training_info
                    WHERE training_metatable_oid = '{metatable_name}'::regclass
                    """.format(
                        madlib_schema = madlib_schema, 
                        metatable_name = metatable_name
                    )
                )
            ref_count = util.__get_query_value(t, "count")

            # if the metatable is not referenced by other training procedure.
            if ref_count == 1 :
                preproc.__drop_metatable(madlib_schema, metatable_name)
                plpy.execute(
                    "DROP TABLE IF EXISTS " + \
                    __get_encode_table_name(madlib_schema, model_table_name)
                )

        # remove the record first, and then drop the table
        __delete_traininginfo(madlib_schema, model_table_name)
        
        plpy.execute("DROP TABLE IF EXISTS " + model_table_name)
    else :
        plpy.execute("DROP TABLE IF EXISTS " + model_table_name)

    return True

 
def __check_dt_common_params (
    madlib_schema               ,
    split_criterion             ,
    training_table_name         ,
    result_table_name           ,
    continuous_feature_names    ,
    feature_col_names           ,
    id_col_name                 ,
    class_col_name              ,
    how2handle_missing_value    ,
    max_tree_depth              ,
    node_prune_threshold        ,
    node_split_threshold        ,
    verbosity                   ,
    error_msg                   
):
    """
    @brief Validate the common parameters for C4.5 and RF API.

    @param split_criterion           The name of the split criterion that 
                                     should be used for tree construction. The
                                     valid values are ‘infogain’, ‘gainratio’, 
                                     and ‘gini’. It can't be NULL.
    @param training_table_name       The name of the table/view with the source
                                     data.
    @param result_table_name         The name of the table where the resulting
                                     DT will be kept.
    @param continuous_feature_names  A comma-separated list of the names of 
                                     features whose values are continuous. The 
                                     default is null, which means there are
                                     no continuous features in the training 
                                     table.

    @param feature_col_names         A comma-separated list of the names of 
                                     table columns, each of which defines a 
                                     feature. The default value is null, which
                                     means all the columns in the training 
                                     table, except columns named
                                     ‘id’ and ‘class’, will be used as features.
    @param id_col_name               The name of the column containing an ID for
                                     each record.
    @param class_col_name            The name of the column containing the 
                                     labeled class.
    @param how2handle_missing_value  The way to handle missing value. The valid 
                                     value is 'explicit' or 'ignore'.
    @param max_tree_depth            Specifies the maximum number of levels in 
                                     the result DT to avoid overgrown DTs.
    @param node_prune_threshold      The minimum percentage of the number of 
                                     records required in a child node. It can't 
                                     be NULL. The range of it is in [0.0, 1.0].
                                     This threshold only applies to the non-root 
                                     nodes. Therefore,
                                     if its value is 1, then the trained tree 
                                     only has one node (the root node);
                                     if its value is 0, then no nodes will be 
                                     pruned by this parameter.
    @param node_split_threshold      The minimum percentage of the number of 
                                     records required in a
                                     node in order for a further split to be 
                                     possible.
                                     It can't be NULL. The range of it is in 
                                     [0.0, 1.0].
                                     If it's value is 1, then the trained tree 
                                     only has two levels, since only the root 
                                     node can grow; if its value is 0, then 
                                     trees can grow extensively.
    @param verbosity                 > 0 means this function runs in verbose 
                                     mode.
    @param error_msg                 The reported error message when 
                                     result_table_name is invalid.
    """
    util.__assert(
        split_criterion is not None             and
        (
            split_criterion == 'infogain' or 
            split_criterion == 'gainratio' or 
            split_criterion == 'gini'
        ),
        "split_criterion must be infogain, gainratio or gini"
    )

    util.__assert(
        how2handle_missing_value == 'ignore'    or 
        how2handle_missing_value == 'explicit',
        "how2handle_missing_value must be ignore or explicit!"
    )

    util.__assert(
        node_prune_threshold is not None        and 
        node_prune_threshold >= 0.0             and 
        node_prune_threshold <= 1.0,
        "node_prune_threshold value must be in range from 0 to 1"
    )

    util.__assert(
        node_split_threshold is not None        and 
        node_split_threshold >= 0.0             and 
        node_split_threshold <= 1.0,
        "node_split_threshold value must be in range from 0 to 1"
    )

    util.__assert(verbosity is not None, "verbosity must be non-null")

    util.__assert(
        id_col_name is not None                 and 
        class_col_name is not None              and 
        len(id_col_name.strip()) > 0            and 
        len(class_col_name.strip()) > 0,
        "invalid id column name or class column name"
    )

    util.__assert(
        training_table_name is not None         and 
        util.__table_exists(madlib_schema, training_table_name),
        "the specified training table <{training_table_name}> " + \
        "does not exist".format(
            training_table_name = training_table_name
        )
    )

    util.__assert(
        result_table_name is not None,
        "the specified result " + error_msg + \
        " table name is NULL"    
    )

    util.__assert(
        not util.__table_exists(madlib_schema, result_table_name),
        "the specified result " + error_msg + " table<" + \
        result_table_name + "> exists"
    )

        
def __gen_enc_meta_names(
    madlib_schema, 
    result_table_name,
    error_msg
):
    """
    @brief Get the name of the encoded table and the name of
    its meta table.
    @param result_table_name   The name of the table where the
    resulting DT will be kept
    @param error_msg           The reported error message when the
    length of result schema name plus
    the length of result table name is
    larger than 58.
    @return A text array that contains two elements. The firest element
    is the encoded table name and the second is the meta table name.
    """
    table_names = ["", ""]
    t = plpy.execute(
            """
            SELECT {madlib_schema}.__get_schema_name
            ('{result_table_name}') AS schema_name         
            """.format(
                madlib_schema = madlib_schema,
                result_table_name = result_table_name
            )
        )

    result_schema_name = util.__get_query_value(t, "schema_name")

    """
    the maximum length of an identifier 63
    encoding table name convension:  <schema name>_<table name>_ed
    data info table name convension: <schema name>_<table name>_di
    the KV table name convension:    <schema name>_<table name>_<####>
    therefore, the maximum length of '<schema name>_<table name>' is 58
    """
    util.__assert(
        len(result_schema_name + "_" + result_table_name) <= 58,
        "the maximum length of '" + error_msg + "' is 58"
    )

    # the encoded table and meta table will be under the specified schema
    table_names[0] = result_schema_name + "." + \
            result_table_name.replace(".", "_") + "_ed"
    table_names[1] = result_schema_name + "." + \
            result_table_name.replace(".", "_") + "_di"
   
    return table_names


# -------------------------------------------
# @brief Validate if the provided columns are in the training table or not.
#
# @param training_table_name       The name of the table/view with the source data.
# @param continuous_feature_names  A text array that contains all the continuous
#                                  features' names.
# @param feature_col_names         A text array that contains all the features' names.
# @param id_col_name               The name of the column containing an ID for each record.
# @param class_col_name            The name of the column containing the labeled class.
# @param features_per_node         The number of features to be considered when finding
#                                                                      a best split.
#
# -------------------------------------------
def __check_training_table(
    madlib_schema               ,
    training_table_name         ,
    continuous_feature_names    ,
    feature_col_names           ,
    id_col_name                 ,
    class_col_name              ,
    features_per_node           
):
    """
    @brief Validate if the provided columns are in the training table or not.
    
    @param training_table_name       The name of the table/view with the source
                                     data.
    @param continuous_feature_names  A text array that contains all the 
                                     continuous features' names.
    @param feature_col_names         A text array that contains all the 
                                     features' names.
    @param id_col_name               The name of the column containing an ID 
                                     for each record.
    @param class_col_name            The name of the column containing the 
                                     labeled class.
    @param features_per_node         The number of features to be considered
                                     when finding a best split.
    """
    util.__assert(
        util.__column_exists(
            madlib_schema,
            training_table_name,
            id_col_name.strip().lower()
        ), 
        "the specified training table <" +training_table_name + \
        "> does not have column " + id_col_name
    )

    util.__assert(
        util.__column_exists(
            madlib_schema,
            training_table_name,
            class_col_name.strip().lower()
        ), 
        "the specified training table <" +training_table_name + \
        "> does not have column " + class_col_name
    )

    if feature_col_names is None :
        # 2 means the id and class column
        util.__assert(
            features_per_node is None or
            preproc.__num_of_columns(
                madlib_schema, 
                training_table_name
            ) - 2 >= features_per_node,
            "the value of features_per_node must be less than or " + \
            "equal to the total number of features of the training table"
        )

        util.__assert(
            util.__columns_in_table(
                madlib_schema,
                continuous_feature_names,
                training_table_name
            ),
            "each feature in continuous_feature_names must be a " + \
            "column of the training table"
        )
    else :
        util.__assert(
            features_per_node is None or 
            len(feature_col_names) >= features_per_node,
            "the value of features_per_node must be less than or " + \
            "equal to the total number of features of the training table"
        )

        util.__assert(
            util.__columns_in_table(
                madlib_schema,
                feature_col_names,
                training_table_names,
            ),
            "each feature in feature_col_names must be a " + \
            "column of the training table"
        )

        util.__assert(
            set(continuous_feature_names) <= set(feature_col_names),
            "each feature in continuous_feature_names must be in the" + \
            " feature_col_names"            
        )


# -------------------------------------------
# @ brief If the training table is a valid encoded table, then we use it directly.
#         If the training table is not encoded, then we invoke the encoding procedure
#         to transform the training table.
#         With the encoded table, we call the tree grow engine to generate the final tree.
#
# @param dt_algo_name                The name of the algorithom. Currently, it's
#                                    'C4.5' or 'RF'
# @param split_criterion             This parameter specifies which split criterion
#                                    should be used for tree construction and
#                                    pruning. The valid values are infogain,
#                                    gainratio, and gini.
# @param num_trees                   Total number of trees to be trained.
# @param features_per_node           Total number of features used to compute split
#                                    gain for each node.
# @param training_table_name         The name of the table/view with the source data.
# @param validation_table_name       The name of the validation table.
# @param tree_table_name             The name of the table where the resulting
#                                    DT/RF will be stored.
# @param continuous_feature_names    A comma-separated list of the names of features whose values
#                                    are continuous. The default is null, which means there are
#                                    no continuous features in the training table.
# @param feature_col_names           A comma-separated list of the names of table columns, each of
#                                    which defines a feature. The default value is null, which means
#                                    all the columns in the training table, except columns named
#                                   ‘id’ and ‘class’, will be used as features.
# @param id_col_name                 The name of the column containing id of each point.
# @param class_col_name              The name of the column containing correct class
#                                    of each point. 
# @param confidence_level            A statistical confidence interval of the
#                                    resubstitution error.
# @param how2handle_missing_value    The way to handle missing value. The valid value
#                                    is 'explicit' or 'ignore'.
# @param max_tree_depth              Maximum decision tree depth.
# @param sampling_percentage         The percentage of records sampled to train a tree.
#                                    If it's NULL, 0.632 bootstrap will be used
# @param sampling_needed             Whether enabling the sampling functionality.
# @param node_prune_threshold        Specifies the minimum number of samples required
#                                    in a child node. 
# @param node_split_threshold        Specifies the minimum number of samples required
#                                    in a node in order for a further split
#                                    to be possible. 
# @param error_msg                   The reported error message when the result table
#                                    name is invalid.
# @param verbosity                   > 0 means this function runs in verbose mode.
#
# @return An instance of __train_result.
# -------------------------------------------
def __encode_and_train(
    madlib_schema               ,
    dt_algo_name                ,
    split_criterion             ,
    num_trees                   ,
    features_per_node           ,
    training_table_name         ,
    validation_table_name       ,
    tree_table_name             ,
    continuous_feature_names    ,
    feature_col_names           ,
    id_col_name                 ,
    class_col_name              ,
    confidence_level            ,   
    how2handle_missing_value    ,
    max_tree_depth              ,
    sampling_percentage         ,
    sampling_needed             ,
    node_prune_threshold        ,
    node_split_threshold        ,
    error_msg                   ,
    verbosity    
):
    """
    @ brief If the training table is a valid encoded table, then we use it 
            directly.
            If the training table is not encoded, then we invoke the encoding
            procedure to transform the training table.
            With the encoded table, we call the tree grow engine to generate 
            the final tree.

    @param dt_algo_name                The name of the algorithom. Currently, 
                                       it's 'C4.5' or 'RF'
    @param split_criterion             This parameter specifies which split 
                                       criterion should be used for tree 
                                       construction and pruning. The valid 
                                       values are infogain, gainratio, and gini.
    @param num_trees                   Total number of trees to be trained.
    @param features_per_node           Total number of features used to compute
                                       split gain for each node.
    @param training_table_name         The name of the table/view with the 
                                       source data.
    @param validation_table_name       The name of the validation table.
    @param tree_table_name             The name of the table where the resulting
                                       DT/RF will be stored.
    @param continuous_feature_names    A comma-separated list of the names of 
                                       features whose values are continuous. 
                                       The default is null, which means there 
                                       are no continuous features in the 
                                       training table.
    @param feature_col_names           A comma-separated list of the names of 
                                       table columns, each of which defines a 
                                       feature. The default value is null, which
                                       means all the columns in the training 
                                       table, except columns named ‘id’ and 
                                       ‘class’, will be used as features.
    @param id_col_name                 The name of the column containing id of
                                       each point.
    @param class_col_name              The name of the column containing correct
                                       class of each point. 
    @param confidence_level            A statistical confidence interval of the
                                       resubstitution error.
    @param how2handle_missing_value    The way to handle missing value. The 
                                       valid value is 'explicit' or 'ignore'.
    @param max_tree_depth              Maximum decision tree depth.
    @param sampling_percentage         The percentage of records sampled to 
                                       train a tree.
                                       If it's NULL, 0.632 bootstrap will be 
                                       used
    @param sampling_needed             Whether enabling the sampling 
                                       functionality.
    @param node_prune_threshold        Specifies the minimum number of samples 
                                       required in a child node. 
    @param node_split_threshold        Specifies the minimum number of samples 
                                       required in a node in order for a further
                                       split to be possible. 
    @param error_msg                   The reported error message when the 
                                       result table name is invalid.
    @param verbosity                   > 0 means this function runs in verbose 
                                       mode.

    @return An instance of __train_result.
    """
    table_names = ['',''] # 0: encoded table; 1: meta table
    h2hmv_routine_id = 1
    h2hmv_routine_name = ""
    enc_tree_name = ""
    cont_feature_col_names = []
    feature_name_array = []
    class train_rs : pass #train result type

    cont_feature_col_names = util.__str_to_array(continuous_feature_names, ',')
    feature_name_array = util.__str_to_array(feature_col_names, ',')
    
    # if the training table is an valid encoded table, then we retrieve
    # the relevant information from training_info table directly.
    if __is_valid_enc_table(madlib_schema, training_table_name) :
        enc_tree_name = __get_tree_table_name(
                            madlib_schema, 
                            training_table_name
                        )
        table_names[0] = training_table_name
        table_names[1] = __get_metatable_name(madlib_schema, enc_tree_name)
        h2hmv_routine_name = __get_routine_name(madlib_schema, enc_tree_name)

        if h2hmv_routine_name == "ignore" :
            h2hmv_routine_id = 1
        else :
            h2hmv_routine_id = 2
       
        # validate the metatable
        preporc.__validate_metatable(madlib_schema, table_names[1])

        n_fids = preproc.__num_of_feature(madlib_schema, table_names[1])
        util.__assert(
            features_per_node is None or n_fids >= features_per_node,
            "the value of features_per_node must be less than or equal to the " \
            "total number of features of the training table"
        )

        # create tree table and auxiliary tables
        # so that we can get the schema name of the table
        __create_tree_tables(madlib_schema, tree_table_name)
    else :
        # the provided columns must be in the training table
        __check_training_table(
            madlib_schema,
            training_table_name,
            cont_feature_col_names,
            feature_name_array,
            id_col_name,
            class_col_name,
            features_per_node
        )

        h2hmv_routine_name = how2handle_missing_value.strip()

        if how2handle_missing_value == 'ignore' :
            h2hmv_routine_id = 1
        else :
            h2hmv_routine_id = 2

        # create tree table and auxiliary tables
        # so that we can get the schema name of the table
        __create_tree_tables(madlib_schema, tree_table_name)

        # encode the training table
        table_names = __gen_enc_meta_names(
                          madlib_schema,
                          tree_table_name,
                          error_msg
                      )
        
        preproc.__encode_table_training(
            madlib_schema,
            training_table_name,
            id_col_name, 
            feature_name_array, 
            class_col_name,
            cont_feature_col_names,
            table_names[0],
            table_names[1], 
            h2hmv_routine_id, 
            verbosity
        )
        
        curstmt = """
                  SELECT {madlib_schema}.__encode_table
                  (
                      '{training_table_name}',
                      '{id_col_name}',
                      {feature_name_array},
                      '{class_col_name}',
                      {cont_feature_col_names},
                      '{table_name_encode}',
                      '{table_name_meta}',
                      {h2hmv_routine_id},
                      {verbosity}
                  )
                  """.format(
                      madlib_schema = madlib_schema, 
                      training_table_name = training_table_name,
                      id_col_name = id_col_name,
                      feature_name_array = 
                          util.__list_to_sql(feature_name_array),
                      class_col_name = class_col_name,
                      cont_feature_col_names = 
                          util.__list_to_sql(cont_feature_col_names),
                      table_name_encode = table_names[0],
                      table_name_meta = table_names[1],
                      h2hmv_routine_id = h2hmv_routine_id,
                      verbosity = verbosity
                  )
        n_fids = preproc.__num_of_feature(madlib_schema, table_names[1])
        
    if sampling_needed :
        if features_per_node is None :
            n_fids = round(math.sqrt(n_fids) - 0.5) + 1
        else :
            n_fids = features_per_node

    if verbosity > 0 :
        plpy.info("features_per_node: " + str(n_fids))
    
    # insert data to the training_info table    
    __insert_into_traininginfo(
        madlib_schema,
        dt_algo_name,
        tree_table_name,
        training_table_name,
        table_names[1],
        table_names[0],
        validation_table_name,
        h2hmv_routine_name,
        split_criterion,
        sampling_percentage,
        n_fids,
        num_trees
    )
    
    # call the tree grow engine
    train_rs = __train_tree(
                   madlib_schema,
                   split_criterion,
                   num_trees,
                   n_fids ,
                   table_names[0],
                   table_names[1],
                   tree_table_name,
                   validation_table_name,
                   'id',
                   'class',
                   confidence_level, 
                   max_tree_depth,
                   sampling_percentage,
                   node_prune_threshold,
                   node_split_threshold,
                   sampling_needed,
                   h2hmv_routine_id,
                   verbosity
               )   
    
    return train_rs
