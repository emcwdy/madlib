/* ----------------------------------------------------------------------- *//**
 *
 * @file dt.sql_in
 *
 * @brief the common functions written in PL/PGSQL shared by C4.5 and RF
 * @date April 5, 2012
 *
 *//* ----------------------------------------------------------------------- */

m4_include(`SQLCommon.m4')

/* Own macro definitions */
m4_ifelse(
    m4_eval(
        m4_ifdef(`__GREENPLUM__', 1, 0) &&
        __DBMS_VERSION_MAJOR__ * 100 + __DBMS_VERSION_MINOR__ < 401
    ), 1,
    `m4_define(`__GREENPLUM_PRE_4_1__')'
)
m4_ifelse(
    m4_eval(
        m4_ifdef(`__POSTGRESQL__', 1, 0) &&
        __DBMS_VERSION_MAJOR__ < 9
    ), 1,
    `m4_define(`__POSTGRESQL_PRE_9_0__')'
)

m4_ifelse(
    m4_eval(
        m4_ifdef(`__GREENPLUM__', 1, 0) &&
        __DBMS_VERSION_MAJOR__ * 10000 +
            __DBMS_VERSION_MINOR__ * 100 +
            __DBMS_VERSION_PATCH__ >= 40201
    ), 1,
    `m4_define(`__GREENPLUM_GE_4_2_1__')'
)

/*
 * This is a global table to store information for various tree training.
 *
 *   classifier_name             The name of the classifier, e.g, 'C4.5' or 'RF'.
 *   result_table_oid            The OID of the result table.
 *   training_table_oid          The OID of the training table.
 *   training_metatable_oid      The OID of the metadata table.
 *   training_encoded_table_oid  The OID of the encoded table.
 *   validation_table_oid        The OID of the validation table.
 *   how2handle_missing_value    The approach name to handle missing value.
 *   split_criterion             The name of the split criterion for this training.
 *   sampling_percentage         The sampling percentage for training each tree.
 *   num_feature_chosen          The number of features will be chosen to find best split.
 *   num_trees                   The number of trees will be grow in training.
 *
 */
DROP TABLE IF EXISTS MADLIB_SCHEMA.training_info;
CREATE TABLE MADLIB_SCHEMA.training_info
    (
    classifier_name             TEXT NOT NULL,
    result_table_oid            OID NOT NULL,
    training_table_oid          OID,
    training_metatable_oid      OID,
    training_encoded_table_oid  OID,
    validation_table_oid        OID,
    how2handle_missing_value    TEXT,
    split_criterion             TEXT,
    sampling_percentage         FLOAT,
    num_feature_chosen          INT,
    num_trees                   INT,
    PRIMARY KEY (result_table_oid)
    ) m4_ifdef(`__GREENPLUM__', `DISTRIBUTED BY (result_table_oid)');
GRANT SELECT, INSERT, UPDATE, DELETE ON MADLIB_SCHEMA.training_info TO PUBLIC;


/*
 * @brief Remove the trained tree from training info table. 
 *
 * @param tree_table    The full name of the tree table.
 *
 */      
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__delete_traininginfo
    (
    tree_table TEXT 
    ) 
RETURNS void AS $$
    PythonFunctionBodyOnly(`cart', `dt')

    dt.__delete_traininginfo(MADlibSchema, tree_table)   
$$ LANGUAGE PLPYTHONU;


/*
 * @brief Insert the trained tree into training info table. 
 *
 * @param classifier_table_name         The name of the classifier.
 * @param result_table_name             The full name of the training result table.
 * @param training_table_name           The full name of the training table.
 * @param training_metatable_name       The full name of metatable.
 * @param training_encoded_table_name   The full name of the encoded table. 
 * @param validation_table_name         The full name of the validation table.
 * @param how2handle_missing_value      The name of the routine to process unknown 
 *                                      values.
 * @param split_criterion               The name of split criterion.
 * @param sampling_percentage           The percentage of bootstrap samples size in 
 *                                      training dataset.
 * @param num_features_chosen           The number of features to split on each tree
 *                                      node. 
 * @param num_trees                     The number of trees after completed the 
 *                                      training process.
 * 
 */ 
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__insert_into_traininginfo
    (
    classifier_table_name       TEXT,
    result_table_name           TEXT,
    training_table_name         TEXT,
    training_metatable_name     TEXT,
    training_encoded_table_name TEXT,
    validation_table_name       TEXT,
    how2handle_missing_value    TEXT,
    split_criterion             TEXT,
    sampling_percentage         FLOAT,
    num_features_chosen         INT,
    num_trees                   INT
    )
RETURNS void AS $$
    PythonFunctionBodyOnly(`cart', `dt')
    
    dt.__insert_into_traininginfo(MADlibSchema, classifier_table_name, result_table_name, training_table_name, 
           training_metatable_name, training_encoded_table_name ,validation_table_name, how2handle_missing_value,
           split_criterion, sampling_percentage, num_features_chosen, num_trees)
$$ LANGUAGE PLPYTHONU;


/*
 * @brief Get the name of the encoded table.  
 *
 * @param tree_table    The full name of the tree table.
 *
 * @return The full name of the encoded table.
 *
 */  
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__get_encode_table_name
    (
    tree_table TEXT 
    ) 
RETURNS TEXT AS $$
    PythonFunctionBodyOnly(`cart', `dt')

    return dt.__get_encode_table_name(MADlibSchema, tree_table)
$$ LANGUAGE PLPYTHONU STABLE;


/*
 * @brief Test if the given table is a valid encoded one. 
 *        A valid encoded table has the following characteristic:
 *            + Its OID is in the column "training_encoded_table_oid"
 *              of training_info table.
 *            + It has 5 columns, whose names are id, fid, fval,
 *              is_cont and class.
 *            + The types of the 5 columns are BIGINT, INT, FLOAT8
 *              BOOL and INT.
 *
 * @param enc_tbl_name    The full name of the encoded table.
 *
 * @return Ture if the given table is a valid encoded one.
 *         False if it's an invalid encoded table.
 *
 */  
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__is_valid_enc_table
    (
    enc_tbl_name TEXT 
    ) 
RETURNS BOOL AS $$
    PythonFunctionBodyOnly(`cart', `dt')
   
    return dt.__is_valid_enc_table(MADlibSchema, enc_tbl_name)
$$ LANGUAGE PLPYTHONU;


/*
 * @brief Get the meta table name by the tree table name. 
 *
 * @param tree_table    The full name of the tree table.
 * 
 * @return The full name of the metatable.
 *
 */      
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__get_metatable_name
    (
    tree_table TEXT 
    ) 
RETURNS TEXT AS $$
    PythonFunctionBodyOnly(`cart', `dt')

    return dt.__get_metatable_name(MADlibSchema, tree_table)
$$ LANGUAGE PLPYTHONU;

/*
 * @brief Get the unknown values processing routine id. 
 *
 * @param tree_table    The full name of the tree table.
 *
 * @return The encoded missing value processing routine id.
 *
 */     
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__get_routine_id
    (
    tree_table TEXT 
    ) 
RETURNS INT AS $$
    PythonFunctionBodyOnly(`cart', `dt')
    
    return dt.__get_routine_id(MADlibSchema, tree_table)
$$ LANGUAGE PLPYTHONU;


/*
 * @brief Get the unknown values processing routine name. 
 *        The valid routine name is 'ignore' or 'explicit'.
 *
 * @param tree_table    The full name of the tree table.
 *
 * @return The encoded missing value processing routine name.
 *
 */     
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__get_routine_name
    (
    tree_table TEXT 
    ) 
RETURNS TEXT AS $$
    PythonFunctionBodyOnly(`cart', `dt')

    return dt.__get_routine_name(MADlibSchema, tree_table)
$$ LANGUAGE PLPYTHONU;


/*
 * @brief Get the name of the tree table from the encoded table name. 
 *
 * @param enc_table_name  The encoded table name.  
 *
 * @return The full name of the tree table.
 *
 */   
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__get_tree_table_name
    (
    enc_table_name TEXT 
    ) 
RETURNS TEXT AS $$
    PythonFunctionBodyOnly(`cart', `dt')

    return dt.__get_tree_table_name(MADlibSchema, enc_table_name)
$$ LANGUAGE PLPYTHONU;


CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__best_scv_sfunc
    (
    result      FLOAT8[],    -- intermediate result
    scv         FLOAT8[],
    fid         INT,
    split_value FLOAT8
    ) 
RETURNS FLOAT8[]  
AS 'MODULE_PATHNAME', 'dt_best_scv_sfunc'
LANGUAGE C STRICT IMMUTABLE;


CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__best_scv_prefunc
    (
    sfunc1_result     FLOAT8[],
    sfunc2_result     FLOAT8[]
    ) 
RETURNS FLOAT8[]
AS 'MODULE_PATHNAME', 'dt_best_scv_prefunc'
LANGUAGE C STRICT IMMUTABLE;


DROP AGGREGATE IF EXISTS MADLIB_SCHEMA.__best_scv_aggr
    (
    FLOAT8[],       -- scv
    INT,            -- fid
    FLOAT8          -- split_value    
    ) CASCADE;
CREATE
AGGREGATE MADLIB_SCHEMA.__best_scv_aggr
    (
    FLOAT8[],       -- scv
    INT,            -- fid
    FLOAT8          -- split_value
    ) 
(
  SFUNC=MADLIB_SCHEMA.__best_scv_sfunc,
  m4_ifdef(`__GREENPLUM__', `prefunc=MADLIB_SCHEMA.__best_scv_prefunc,')
  STYPE=FLOAT8[],
  initcond = '{0, 0, 0, 0, 0, 0, 0}'
);


/*
 * @brief The step function is defined to process each record in the ACS set. 
 *        The records have this format: 
 *        {fid, fval, is_cont, split_value, le, total, tid, nid}
 *
 * @param result            The array used to keep the best attribute's info.
 * @param sc_code           The code of the split criterion.
 * @param is_cont           True  - The feature is continuous. 
 *                          False - The feature is discrete.
 * @param num_class         The total number of classes.
 * @param le_array          The le component of the ACS record. le_array[i] is the 
 *                          number of samples whose class code equals to i and 
 *                          whose fval is less-than or equal to the fval component 
 *                          of the ACS record being processed.
 * @param total_array       The total component of the ACS record. total_array[i] is
 *                          the number of samples whose class code equals to i.
 * @param true_total        The real total number of samples currently assigned to 
 *                          the node identified by (tid, nid). If there are missing
 *                          values in fval, the sum of all elements in total_array
 *                          will be less than true_total.
 *
 * @return A 9-element array. Please refer to the definition of SCV_STATE_ARRAY_INDEX
 *         in dt.c for the detailed information of this array.
 *
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__scv_aggr_sfunc
    (
    result          FLOAT8[],
    sc_code         INT,
    is_cont         BOOLEAN,
    num_class       INT,
    le_array        FLOAT8[],
    total_array     FLOAT8[],
    true_total      BIGINT
    )
RETURNS FLOAT8[]
AS 'MODULE_PATHNAME', 'dt_scv_aggr_sfunc'
LANGUAGE C IMMUTABLE;


/*
 * @brief The pre-function for the aggregation of splitting criteria values. It  
 *        takes the state array produced by two sfunc and combine them together.
 *
 * @param sfunc1_result     The array from sfunc1.
 * @param sfunc2_result     The array from sfunc2.
 *
 * @return A 9-element array. Please refer to the definition of SCV_STATE_ARRAY_INDEX
 *         in dt.c for the detailed information of this array.
 *
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__scv_aggr_prefunc
    (
    sfunc1_result     FLOAT8[],
    sfunc2_result     FLOAT8[]
    ) 
RETURNS FLOAT8[]
AS 'MODULE_PATHNAME', 'dt_scv_aggr_prefunc'
LANGUAGE C STRICT IMMUTABLE;


/*
 * @brief The final function for the aggregation of splitting criteria values.
 *        It takes the state array produced by the sfunc and produces a
 *        5-element array.
 *
 * @param internal_result   The 9-element array produced by dt_scv_aggr_prefunc
 *
 * @return A 5-element array. Please refer to the definition of SCV_FINAL_ARRAY_INDEX
 *         in dt.c for the detailed information of this array.
 *
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__scv_aggr_ffunc
    (
    internal_result     FLOAT8[]
    ) 
RETURNS FLOAT8[]
AS 'MODULE_PATHNAME', 'dt_scv_aggr_ffunc'
LANGUAGE C STRICT IMMUTABLE;


DROP AGGREGATE IF EXISTS MADLIB_SCHEMA.__scv_aggr
    (
    INT,        -- sc
    BOOLEAN,    -- is_cont
    INT,        -- total number of classes
    FLOAT8[],   -- le array
    FLOAT8[],   -- total count array
    BIGINT      -- the total number of samples
    ) CASCADE;
CREATE
AGGREGATE MADLIB_SCHEMA.__scv_aggr
    (
    INT,        -- sc
    BOOLEAN,    -- is_cont
    INT,        -- total number of classes
    FLOAT8[],   -- le array
    FLOAT8[],   -- total count array
    BIGINT      -- the total number of samples
    ) 
(
  SFUNC=MADLIB_SCHEMA.__scv_aggr_sfunc,
  m4_ifdef(`__GREENPLUM__', `prefunc=MADLIB_SCHEMA.__scv_aggr_prefunc,')
  FINALFUNC=MADLIB_SCHEMA.__scv_aggr_ffunc,
  STYPE=FLOAT8[],
  initcond = '{0, 0, 0, 0, 0, 0, 0, 0, 0}'
  -- 1   sc: 1 infogain, 2 gainratio, 3 gini
  -- 2   is_cont
  -- 3   scv_class_info
  -- 4   scv_attr_info
  -- 5   scv_class_attr_info
  -- 6   scv_count
  -- 7   scv_total
  -- 8   max_class_id
  -- 9   max_class_count
);


/*
 * @brief Retrieve the specified number of unique features for a node.
 *        Discrete features used by ancestor nodes will be excluded.
 *        If the number of remaining features is less or equal than the
 *        requested number of features, then all the remaining features
 *        will be returned. Otherwise, we will sample the requested 
 *        number of features from the remaining features.
 *
 * @param num_req_features  The number of requested features.
 * @param num_features      The total number of features.
 * @param nid               The ID of the node for which the
 *                          features are sampled.
 * @param dp_fids           The IDs of the discrete features
 *                          used by the ancestors.
 *
 * @return An array containing all the IDs of chosen features.
 *
 */
CREATE OR REPLACE FUNCTION 
MADLIB_SCHEMA.__dt_get_node_split_fids(INT4, INT4, INT4, INT4[])
RETURNS INT[]
AS 'MODULE_PATHNAME', 'dt_get_node_split_fids'
LANGUAGE C VOLATILE;


/*
 * @brief Retrieve the selected features for a node. We will create a table, named 
 *        sf_association, to store the association between selected feature IDs and
 *        node IDs.
 *
 * @param nid_table_name    The full name of the table which contains all the 
 *                          node IDs.
 * @param result_table_name The full name of the table which contains the parent
 *                          discrete features for each node.
 * @param num_chosen_fids   The number of feature IDs will be chosen for a node.
 * @param total_num_fids    The total number of feature IDs, total_num_fids 
 *                          >= num_chosen_fids.
 *                          If num_chosen_fids < total_num_fids, then we will 
 *                          randomly select num_chosen_fids features from all
 *                          the features. Otherwise, we will return all the  
 *                          features exception they belong to the parent discrete
 *                          features for a node.
 * @param verbosity         > 0 means this function runs in verbose mode.
 *                    
 * @return An constant string for the association table name.
 *
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__get_features_of_nodes
    (
    nid_table_name        TEXT,
    result_table_name     TEXT,
    num_chosen_fids       INT,
    total_num_fids        INT,
    verbosity             INT
    )
RETURNS TEXT AS $$
    PythonFunctionBodyOnly(`cart', `dt')
 
    return dt.__get_features_of_nodes(MADlibSchema, nid_table_name, result_table_name, num_chosen_fids,
                  total_num_fids, verbosity)
$$ LANGUAGE PLPYTHONU;


/*
 * This UDT is used to keep the times of generating acc.
 *
 * calc_pre_time   The time of pre-processing.
 * calc_acc_time   The time of calculating acc.
 *
 */
DROP TYPE IF EXISTS MADLIB_SCHEMA.__gen_acc_time;
CREATE TYPE MADLIB_SCHEMA.__gen_acc_time AS 
(   
    calc_pre_time       INTERVAL,
    calc_acc_time       INTERVAL
);


/*
 * @brief Generate the ACC for current leaf nodes.
 *
 * @param encoded_table_name    The full name of the encoded table for the  
 *                              training table.
 * @param metatable_name        The full name of the metatable contains the  
 *                              relevant information of the input table.
 * @param result_table_name     The full name of the training result table.
 * @param num_featrue_try       The number of features will be chosen per node. 
 * @param num_classes           Total number of classes in training set.
 * @param verbosity             > 0 means this function runs in verbose mode. 
 *                    
 * @return The time information for generating ACC.
 *
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__gen_acc
    (
    encoded_table_name      TEXT,
    metatable_name          TEXT,
    result_table_name       TEXT,
    tr_table_name           TEXT,
    sf_table_name           TEXT,
    num_featrue_try         INT,
    num_classes             INT,
    sampling_needed         BOOLEAN,
    verbosity               INT
    )
RETURNS MADLIB_SCHEMA.__gen_acc_time AS $$
    PythonFunctionBodyOnly(`cart', `dt')

    return dt.__gen_acc(MADlibSchema, encoded_table_name, metatable_name, result_table_name, tr_table_name,
                  sf_table_name, num_featrue_try, num_classes, sampling_needed, verbosity)
$$ LANGUAGE PLPYTHONU;


DROP TYPE IF EXISTS MADLIB_SCHEMA.__rep_type CASCADE;
CREATE TYPE MADLIB_SCHEMA.__rep_type AS
    (
    numOfOrgClasses BIGINT[]
    );


/*
 * @brief The step function for aggregating the class counts while doing Reduce 
 *        Error Pruning (REP).
 *
 * @param class_count_array     The array used to store the accumulated information.
 *                              [0]: the total number of mis-classified samples.
 *                              [i]: the number of samples belonging to the ith class.
 * @param classified_class      The predicted class based on our trained DT model.
 * @param original_class        The real class value provided in the validation set.
 * @param max_num_of_classes    The total number of distinct class values. 
 *                    
 * @return An updated class count array.
 *
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__rep_aggr_class_count_sfunc
    (
    class_count_array       BIGINT[],        
    classified_class        INT,
    original_class          INT,
    max_num_of_classes      INT
    ) 
RETURNS BIGINT[]
AS 'MODULE_PATHNAME', 'dt_rep_aggr_class_count_sfunc'
LANGUAGE C IMMUTABLE;


/*
 * @brief Add the corresponding elements of the input arrays 
 *        to create a new one.
 *
 * @param 1 arg     The array 1.
 * @param 2 arg     The array 2.
 *                    
 * @return The new array.
 *
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__bigint_array_add
    (
    BIGINT[],
    BIGINT[]
    ) 
RETURNS BIGINT[]
AS 'MODULE_PATHNAME', 'bigint_array_add'
LANGUAGE C IMMUTABLE;


/*
 * @brief The final function for aggregating the class counts for REP. 
 *        It takes the class count array produced by the sfunc and produces a 
 *        two-element array. The first element is the ID of the class that has 
 *        the maximum number of samples represented by the root node of the subtree
 *        being processed. The second element is the number of reduced  
 *        misclassified samples if the leave nodes of the subtree are pruned.
 *
 * @param class_count_data     The array containing all the information for the 
 *                             calculation of Reduced-Error pruning. 
 *                    
 * @return A two element array.
 *
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__rep_aggr_class_count_ffunc
    (
    class_count_array       BIGINT[]        
    ) 
RETURNS BIGINT[]
AS 'MODULE_PATHNAME', 'dt_rep_aggr_class_count_ffunc'
LANGUAGE C STRICT IMMUTABLE;


DROP AGGREGATE IF EXISTS MADLIB_SCHEMA.__rep_aggr_class_count
    (
    INT,
    INT,
    INT
    );
CREATE AGGREGATE MADLIB_SCHEMA.__rep_aggr_class_count
    (
    INT,
    INT,
    INT
    ) 
(
  SFUNC=MADLIB_SCHEMA.__rep_aggr_class_count_sfunc,
  m4_ifdef(`__GREENPLUM__', `prefunc=MADLIB_SCHEMA.__bigint_array_add,')
  FINALFUNC=MADLIB_SCHEMA.__rep_aggr_class_count_ffunc,
  STYPE=BIGINT[]
);


/*
 * @brief The step function of the aggregate __array_indexed_agg.
 *
 * @param state         The step state array of the aggregate function.
 * @param elem          The element to be filled into the state array.
 * @param elem_cnt      The number of elements.
 * @param elem_idx      the subscript of "elem" in the state array.
 * 
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__array_indexed_agg_sfunc
    (
    state       float8[], 
    elem        float8, 
    elem_cnt    int8, 
    elem_idx    int8
    )
RETURNS float8[]
AS 'MODULE_PATHNAME', 'dt_array_indexed_agg_sfunc' 
LANGUAGE C IMMUTABLE;


/*
 * @brief The Pre-function of the aggregate __array_indexed_agg.
 * 
 * @param arg0  The first state array.
 * @param arg1  The second state array.
 *  
 * @return The combined state.  
 *
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__array_indexed_agg_prefunc
    (
    float8[], 
    float8[]
    )
RETURNS float8[]
AS 'MODULE_PATHNAME', 'dt_array_indexed_agg_prefunc' 
LANGUAGE C STRICT IMMUTABLE;


/*
 * @brief The final function of __array_indexed_agg.
 * 
 * @param state  The state array.
 * 
 * @return The aggregate result.
 *
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__array_indexed_agg_ffunc
    (
    float8[]
    )
RETURNS float8[]
AS 'MODULE_PATHNAME', 'dt_array_indexed_agg_ffunc' 
LANGUAGE C IMMUTABLE;


/*
 * @brief The aggregate is the same with array_agg, which will accumulate
 *        The elements in each group to an array, except that we allow users 
 *        provide the subscript for each element. This aggregate will be 
 *        invoked as HashAggregate, while array_agg will be called as 
 *        GroupAggregate. Therefore, our implementation have better performance
 *        than the array_agg.
 * 
 * @param elem     The element to be fed into the returned array of this aggregate.
 * @param elem_cnt The number of elements.
 * @param elem_idx The subscript of the element.
 *
 * @return The aggregated array.
 *
 */
CREATE AGGREGATE MADLIB_SCHEMA.__array_indexed_agg(float8, int8, int8) (
    SFUNC = MADLIB_SCHEMA.__array_indexed_agg_sfunc,
    m4_ifdef( `GREENPLUM',`PREFUNC   = MADLIB_SCHEMA.__array_indexed_agg_prefunc,')
    FINALFUNC = MADLIB_SCHEMA.__array_indexed_agg_ffunc, 
    STYPE = float8[]
);


CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__dt_acc_count_sfunc
    (
    count_array         BIGINT[],
    num_of_class        INT,
    count               BIGINT,
    class               INT
    ) 
RETURNS BIGINT[]  
AS 'MODULE_PATHNAME', 'dt_acc_count_sfunc'
LANGUAGE C VOLATILE;


CREATE AGGREGATE MADLIB_SCHEMA.__dt_acc_count_aggr
    (
    INT,
    BIGINT,
    INT
    ) 
(
  SFUNC=MADLIB_SCHEMA.__dt_acc_count_sfunc,
  m4_ifdef(`__GREENPLUM__', `prefunc=MADLIB_SCHEMA.__bigint_array_add,')
  STYPE=BIGINT[]
);


/*
 * @brief The aggregate is created for the PostgreSQL, which doesn't support the
 *        function sum over an array.
 * 
 * @param elem     The element to be fed into the returned array of this aggregate.
 *
 * @return The array with the sum of all the input array in a group.
 *
 */
CREATE
AGGREGATE MADLIB_SCHEMA.__bigint_array_sum
    (
    BIGINT[]
    ) 
(
  SFUNC=MADLIB_SCHEMA.__bigint_array_add,
  m4_ifdef(`__GREENPLUM__', `prefunc=MADLIB_SCHEMA.__bigint_array_add,')
  STYPE=BIGINT[]
);


/*
 * @brief This function find the best split and return the information.
 *
 * @param table_name          The name of the table containing the training
 *                            set.
 * @param confidence_level    This parameter is used by the 'Error-Based Pruning'.
 *                            Please refer to the paper for detailed definition.
 *                            The paper's name is 'Error-Based Pruning of Decision  
 *                            Trees Grown on Very Large Data Sets Can Work!'.
 * @param feature_table_name  Is is the name of one internal table, which contains
 *                            meta data for each feature.
 * @param split_criterion     It defines the split criterion to be used.
 *                            (1- information gain. 2- gain ratio. 3- gini).
 * @param continue_grow       It specifies whether we should still grow the tree
 *                            on the selected branch.
 * @param output_table        It specifies the table used to store the chosen splits.
 * @param h2hmv_routine_id    Specifies how to handle missing values. 
 *                            1 ignore, 2 explicit.
 *                    
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__find_best_split
    (
    table_name              TEXT, 
    confidence_level        FLOAT,
    feature_table_name      TEXT, 
    split_criterion         INT, 
    continue_grow           INT,
    output_table            TEXT,
    h2hmv_routine_id        INT,
    num_classes             INT
    ) 
RETURNS VOID AS $$
    PythonFunctionBodyOnly(`cart', `dt')   

    dt.__find_best_split(MADlibSchema, table_name, confidence_level, feature_table_name, 
           split_criterion, continue_grow, output_table, h2hmv_routine_id, num_classes)
$$ LANGUAGE PLPYTHONU;


/*
 * @brief For training one decision tree, we need some internal tables
 *        to store intermediate results. This function creates those
 *        tables. Moreover, this function also creates the tree table
 *        specified by user.
 *
 * @param result_tree_table_name  The name of the tree specified by user. 
 *                    
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__create_tree_tables
    (
    result_tree_table_name TEXT
    ) 
RETURNS void AS $$ 
    PythonFunctionBodyOnly(`cart', `dt')

    dt.__create_tree_tables(MADlibSchema, result_tree_table_name)
$$ LANGUAGE PLPYTHONU;


/*
 * @brief Prune the trained tree with "Reduced Error Pruning" algorithm.
 *
 * @param tree_table_name   The name of the table containing the tree. 
 * @param validation_table  The name of the table containing validation set. 
 * @param max_num_classes   The count of different classes. 
 *                    
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__rep_prune_tree
    (
    tree_table_name     TEXT, 
    validation_table    TEXT, 
    max_num_classes     INT
    ) 
RETURNS void AS $$
    PythonFunctionBodyOnly(`cart', `dt')

    dt.__rep_prune_tree(MADlibSchema, tree_table_name, validation_table, max_num_classes)
$$ LANGUAGE PLPYTHONU;


/*
 * @brief Calculates the total errors used by Error Based Pruning (EBP).
 *
 * @param total             The number of total samples represented by the node 
 *                          being processed. 
 * @param prob              The probability to mis-classify samples represented by the 
 *                          child nodes if they are pruned with EBP. 
 * @param confidence_level  A certainty factor to calculate the confidence limits
 *                          for the probability of error using the binomial theorem. 
 *  
 * @return The computed total error.
 *
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__ebp_calc_errors
    (
    total               FLOAT8,
    prob                FLOAT8,
    confidence_level    FLOAT8
    ) RETURNS FLOAT8 
AS 'MODULE_PATHNAME', 'dt_ebp_calc_errors'
LANGUAGE C STRICT IMMUTABLE;


/*
 * @brief Prune the trained tree with "Error-based Pruning" algorithm.
 *
 * @param tree_table_name  The name of the table containing the tree. 
 *  
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__ebp_prune_tree
    (
    tree_table_name TEXT
    ) 
RETURNS void AS $$
    PythonFunctionBodyOnly(`cart', `dt')

    dt.__ebp_prune_tree(MADlibSchema, tree_table_name)
$$ LANGUAGE PLPYTHONU;


/*
 * @brief Generate the final trained tree.
 *
 * @param result_tree_table_name  The name of the table containing the tree.
 *  
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__generate_final_tree
    (
    result_tree_table_name TEXT
    ) 
RETURNS void AS $$
    PythonFunctionBodyOnly(`cart', `dt')

    dt.__generate_final_tree(MADlibSchema, result_tree_table_name)
$$ LANGUAGE PLPYTHONU;


/*
 * The UDT for the training result.
 *
 *      num_of_samples           It means how many records there exists in the 
 *                               training set.   
 *      features_per_node        The number of features chosen for each tree.
 *      num_tree_nodes           The number of tree nodes.
 *      max_tree_depth           The max tree depth.
 *      calc_acc_time            Total time of calculating acc.
 *      calc_pre_time            Time of preprocessing when calculating acc.
 *      update_time              Total time of updating operation after found
 *                               the best time. 
 *      update_best              Time of updating the best splits' information.
 *      update_child             Time of generating the child nodes.
 *      update_nid               Time of updating the assigned node IDs.
 *      scv_acs_time             Time of calculating the best splits.     
 *      prune_time               Time of tree pruning.
 *
 */
DROP TYPE IF EXISTS MADLIB_SCHEMA.__train_result;
CREATE TYPE MADLIB_SCHEMA.__train_result AS 
(   
    num_of_samples           BIGINT,   
    features_per_node        INT,
    num_tree_nodes           INT,
    max_tree_depth           INT,
    calc_acc_time            INTERVAL,
    calc_pre_time            INTERVAL,
    update_time              INTERVAL,
    update_best              INTERVAL,
    update_child             INTERVAL,
    update_nid               INTERVAL,
    scv_acs_time             INTERVAL,
    prune_time               INTERVAL
);


/*
 * @brief The function samples a set of integer values between low and high.
 *
 * @param num_of_samples  The number of records to be sampled.
 * @param low             The low limit of sampled values.
 * @param high            The high limit of sampled values.
 *
 * @return A set of integer values sampled randomly between [low, high].
 *
 */
DROP FUNCTION IF EXISTS MADLIB_SCHEMA.__sample_within_range
    (
    BIGINT,
    BIGINT,
    BIGINT
    )CASCADE;
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__sample_within_range
    (
    num_of_samples      BIGINT,
    low                 BIGINT,
    high                BIGINT
    ) 
RETURNS SETOF BIGINT  
AS 'MODULE_PATHNAME', 'dt_sample_within_range'
LANGUAGE C STRICT VOLATILE;


/*
 * @brief The function samples with replacement from source table and store
 *        the results to target table.
 * 
 *        In this function, we firstly calculate how many samples should be
 *        generated in each segment. Then, we let those segments sample with
 *        replacement between the maximum ID and minimum ID of the source table 
 *        in parallel and assign samples to different trees. 
 *
 *        If there are gaps in the ID column of the source table, we sample
 *        extra records in proportion to the number of gaps. At last, we remove
 *        these invalid samples with an inner join operation with the source
 *        table. Since we target big data, this strategy works quite well.
 *
 * @param num_of_tree     The number of trees to be trained.
 * @param size_per_tree   The number of records to be sampled for each tree.
 * @param src_table       The name of the table to be sampled from.
 * @param target_table    The name of the table used to store the results.
 *
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__sample_with_replacement
    ( 
    num_of_tree     INT,
    size_per_tree   INT, 
    src_table       TEXT,
    target_table    TEXT
    ) 
RETURNS VOID AS $$
    PythonFunctionBodyOnly(`cart', `dt')
    
    dt.__sample_with_replacement(MADlibSchema, num_of_tree, size_per_tree, src_table, target_table)    
$$ LANGUAGE PLPYTHONU VOLATILE;


/*
 * @brief This function trains a decision tree or random forest.
 *
 * @param split_criterion             This parameter specifies which split criterion 
 *                                    should be used for tree construction and 
 *                                    pruning. The valid values are infogain, 
 *                                    gainratio, and gini.
 * @param num_trees                   Total number of trees to be trained. 
 * @param features_per_node           Total number of features used to compute split 
 *                                    gain for each node. 
 * @param training_table_name         The name of the table/view with the source data. 
 * @param training_table_meta         The name of the table with the meta data. 
 * @param result_tree_table_name      The name of the table where the resulting 
 *                                    DT/RF will be stored. 
 * @param validation_table_name       The validation table used for pruning tree.  
 * @param id_col_name                 The name of the column containing id of each point.  
 * @param class_col_name              The name of the column containing correct class 
 *                                    of each point.  
 * @param confidence_level            A statistical confidence interval of the 
 *                                    resubstitution error.  
 * @param max_tree_depth              Maximum decision tree depth.  
 * @param node_prune_threshold        Specifies the minimum number of samples required 
 *                                    in a child node.  
 * @param node_split_threshold        Specifies the minimum number of samples required 
 *                                    in a node in order for a further split   
 *                                    to be possible.  
 * @param sampling_needed             Whether enabling the sampling functionality.  
 * @param h2hmv_routine_id            Specifies how to handle missing values. 
 *                                    1 ignore, 2 explicit.
 * @param verbosity                   > 0 means this function runs in verbose mode. 
 *  
 * @return The record including training related information.
 *		   Details please refer to the UDT: MADLIB_SCHEMA.__train_result.
 *
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__train_tree
    (
    split_criterion         TEXT,
    num_trees               INT,
    features_per_node       INT,
    training_table_name     TEXT, 
    training_table_meta     TEXT,
    result_tree_table_name  TEXT,
    validation_table_name   TEXT,
    id_col_name             TEXT, 
    class_col_name          TEXT, 
    confidence_level        FLOAT, 
    max_tree_depth          INT,
    sampling_percentage     FLOAT,
    node_prune_threshold    FLOAT,
    node_split_threshold    FLOAT,
    sampling_needed         BOOLEAN,
    h2hmv_routine_id        INT,
    verbosity               INT
    ) 
RETURNS MADLIB_SCHEMA.__train_result AS $$
    PythonFunctionBodyOnly(`cart', `dt')

    return dt.__train_tree(MADlibSchema, split_criterion, num_trees, features_per_node, training_table_name,
                  training_table_meta, result_tree_table_name, validation_table_name, id_col_name,
                  class_col_name, confidence_level, max_tree_depth, sampling_percentage, node_prune_threshold,
                  node_split_threshold, sampling_needed, h2hmv_routine_id, verbosity)
$$ LANGUAGE PLPYTHONU;


/*
 * @brief This is an internal function for displaying one tree node in human  
 *        readable format. It is the step function of aggregation named 
 *        __display_tree_aggr.
 *
 * @param state     This variable is used to store the accumulated tree 
 *                  display information.
 * @param depth     The depth of this node. 
 * @param is_cont   Whether the feature used to split is continuous. 
 * @param feat_name The name of the feature used to split.
 * @param curr_val  The value of the splitting feature for this node.
 * @param split_value    For continuous feature, it specifies the split value. 
 *                  Otherwise, it is of no meaning.
 * @param max_prob  For those elements in this node, the probability that
 *                  an element belongs to the max_class.
 * @param max_class The class ID with the largest number of elements 
 *                  for those elements in this node.
 * @param num_of_samples Total count of samples in this node. 
 *
 * @return It returns the text containing the information of human  
 *         readable information for trees.
 *
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__display_node_sfunc
    (
    state           TEXT,
    depth           INT,
    is_cont         BOOLEAN,
    feat_name       TEXT,
    curr_val        TEXT,
    split_value     FLOAT8,
    max_prob        FLOAT8,
    max_class       TEXT,
    num_of_samples  INT
    ) 
RETURNS TEXT AS $$ 
    PythonFunctionBodyOnly(`cart', `dt')

    return dt.__display_node_sfunc(MADlibSchema, state, depth, is_cont, feat_name, curr_val, split_value,
               max_prob, max_class, num_of_samples)
$$ LANGUAGE PLPYTHONU;


DROP AGGREGATE IF EXISTS MADLIB_SCHEMA.__display_tree_aggr
    (
    INT,        -- depth
    BOOLEAN,    -- is_cont
    TEXT,       -- feature name
    TEXT,       -- curr_val
    FLOAT8,     -- split value
    FLOAT8,     -- max_probability
    TEXT,       -- max_class
    INT         -- num_of_samples
    ) CASCADE;
CREATE 
m4_ifdef(`__GREENPLUM__', m4_ifdef(`__HAS_ORDERED_AGGREGATES__', `ORDERED'))
AGGREGATE MADLIB_SCHEMA.__display_tree_aggr
    (
    INT,        -- depth
    BOOLEAN,    -- is_cont
    TEXT,       -- feature name
    TEXT,       -- curr_val
    FLOAT8,     -- split value
    FLOAT8,     -- max_probability
    TEXT,       -- max_class
    INT         -- num_of_samples
    ) 
(
  SFUNC=MADLIB_SCHEMA.__display_node_sfunc,
  STYPE=TEXT
);


/*
 * @brief Display the trained model with human readable format. This function
 *        leverages ordered aggregate to display the tree with only one scan of
 *        the tree_table.
 *
 * @param tree_table  The full name of the tree table. 
 * @param tree_id     The array contains the IDs of the trees to be displayed.
 * @param max_depth   The max depth to be displayed. If it is set to null,
 *                    this function will show all levels. 
 *
 * @return The text representing the tree with human readable format.
 *
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__treemodel_display_with_ordered_aggr
    (
    tree_table  TEXT,
    tree_id     INT[],    
    max_depth   INT
    ) 
RETURNS SETOF TEXT AS $$
    PythonFunctionBodyOnly(`cart', `dt')
  
    return dt.__treemodel_display_with_ordered_aggr(MADlibSchema, tree_table, tree_id, max_depth)
$$ LANGUAGE PLPYTHONU;


/*
 * @brief This is an internal function for displaying the tree in human readable
 *        format. It uses the depth-first strategy to traverse a tree and print 
 *        values. This function is used on databases, e.g. PG 8.4, that do not 
 *        support ordered aggregate.
 *
 * @param tree_table      The full name of the tree table. 
 * @param id              The ID of current node. This node and all of its  
 *                        children are displayed.
 * @param feature_id      The ID of a feature, which is used to split in the 
 *                        parent of current node.
 * @param depth           The depth of current node.
 * @param is_cont         It specifies whether the feature denoted by 'feature_id'
 *                        is continuous or not.
 * @param split_value     For continuous feature, it specifies the split value. 
 *                        Otherwise, it is of no meaning.
 * @param metatable_name  For tabular format, this table contains the meta data
 *                        to encode the input table.
 * @param max_depth       The max depth to be displayed. If it is set to null,
 *                        this function will show all levels. 
 * @param tree_id         The ID of the tree to be displayed.
 *
 * @return The text representing the tree with human readable format.
 *
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__display_tree_no_ordered_aggr
    (
    tree_table      TEXT, 
    id              INT, 
    feature_id      INT, 
    depth           INT, 
    is_cont         BOOLEAN, 
    split_value     FLOAT,
    metatable_name  TEXT,
    max_depth       INT,
    tree_id         INT
    ) 
RETURNS TEXT AS $$ 
    PythonFunctionBodyOnly(`cart', `dt')

    return dt.__display_tree_no_ordered_aggr(MADlibSchema, tree_table, id, feature_id, depth, 
               is_cont, split_value, metatable_name, max_depth, tree_id)
$$ LANGUAGE PLPYTHONU;


/*
 * @brief Display the trained model with human readable format. It use the 
 *        recursive algorithm, which is slower than the version with 
 *        ordered aggregate. We only use it when ordered aggregate is unavailable.
 *
 * @param tree_table  The full name of the tree table. 
 * @param tree_id     The array contains the IDs of the trees to be displayed.
 * @param max_depth   The max depth to be displayed. If it is set to null,
 *                    this function will show all levels. 
 *
 * @return The text representing the tree with human readable format.
 *
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__treemodel_display_no_ordered_aggr
    (
    tree_table  TEXT,
    tree_id     INT[],
    max_depth   INT
    ) 
RETURNS SETOF TEXT AS $$
    PythonFunctionBodyOnly(`cart', `dt')

    return dt.__treemodel_display_no_ordered_aggr(MADlibSchema, tree_table, tree_id, max_depth)
$$ LANGUAGE PLPYTHONU;


/*
 * @brief Multiple trees may classify the same record to different classes. 
 *        This function gets the results voted by multiple trees.
 *
 * @param src_table    The full name of the table containing original data.
 * @param dst_table    The full name of the table to store the voted results. 
 *
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__treemodel_get_vote_result
    (
    src_table   TEXT, 
    dst_table   TEXT
    )
RETURNS VOID AS $$
    PythonFunctionBodyOnly(`cart', `dt')
 
    dt.__treemodel_get_vote_result(MADlibSchema, src_table, dst_table)
$$ LANGUAGE PLPYTHONU;


/*
 * @brief An internal classification function. It classifies with all trees at 
 *        the same time. For medium/small data sets, tests shows that it is more
 *        efficient than the serial classification function. 
 *
 * @param classification_table_name  The full name of the table containing the 
 *                                   classification set.
 * @param tree_table_name            The full name of the tree table.
 * @param verbosity                  > 0 means this function runs in verbose mode. 
 *
 * @return An array containing the encoded table name and classification result 
 *         table name (We encode the source table during the classification).
 *
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__treemodel_classify_internal
    (
    classification_table_name   TEXT, 
    tree_table_name             TEXT, 
    verbosity                   INT
    ) 
RETURNS TEXT[] AS $$
    PythonFunctionBodyOnly(`cart', `dt')

    return dt.__treemodel_classify_internal(MADlibSchema, classification_table_name, tree_table_name, verbosity)    
$$ LANGUAGE PLPYTHONU;


/*
 * @brief An internal classification function. It classifies with one tree 
 *        after another. For large data sets, tests shows that it is more
 *        efficient than the parallel classification function.   
 *
 * @param classification_table_name  The full name of the table containing the 
 *                                   classification set.
 * @param tree_table_name            The full name of the tree table.
 * @param verbosity                  > 0 means this function runs in verbose mode. 
 *
 * @return An array containing the encoded table name and classification result 
 *         table name (We encode the source table during the classification).
 *
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__treemodel_classify_internal_serial
    (
    classification_table_name   TEXT, 
    tree_table_name             TEXT, 
    verbosity                   INT
    ) 
RETURNS TEXT[] AS $$
    PythonFunctionBodyOnly(`cart', `dt')

    return dt.__treemodel_classify_internal_serial(MADlibSchema, classification_table_name, tree_table_name, verbosity)
$$ LANGUAGE PLPYTHONU;


/*
 * @brief This function check the accuracy of the trained tree model.
 * 
 * @param tree_table_name     The name of the tree containing the model.
 * @param scoring_table_name  The full name of the table/view with the 
 *                            data to be scored.
 * @param verbosity           > 0 means this function runs in verbose mode.
 *
 * @return The estimated accuracy information.
 *
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__treemodel_score
    (
    tree_table_name             TEXT, 
    scoring_table_name          TEXT, 
    verbosity                   INT
    ) 
RETURNS FLOAT AS $$
    PythonFunctionBodyOnly(`cart', `dt')

    return dt.__treemodel_score(MADlibSchema, tree_table_name, scoring_table_name, verbosity)
$$ LANGUAGE PLPYTHONU;

/*
 * @brief Cleanup the trained model table and any relevant tables.
 *
 * @param model_table_name The name of the table containing
 *                         the model's information.
 *
 * @return The status of that cleanup operation.
 *
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__treemodel_clean
    ( 
    model_table_name TEXT
    ) 
RETURNS BOOLEAN AS $$
    PythonFunctionBodyOnly(`cart', `dt')

    return dt.__treemodel_clean(MADlibSchema, model_table_name)
$$ LANGUAGE PLPYTHONU;

/*
 * @brief Validate the common parameters for C4.5 and RF API.
 *
 * @param split_criterion           The name of the split criterion that should be used 
 *                                  for tree construction. The valid values are
 *                                  ‘infogain’, ‘gainratio’, and ‘gini’. It can't be NULL.
 * @param training_table_name       The name of the table/view with the source data.
 * @param result_table_name         The name of the table where the resulting DT 
 *                                  will be kept.
 * @param continuous_feature_names  A comma-separated list of the names of features whose values 
 *                                  are continuous. The default is null, which means there are 
 *                                  no continuous features in the training table.
 * @param feature_col_names         A comma-separated list of the names of table columns, each of
 *                                  which defines a feature. The default value is null, which means 
 *                                  all the columns in the training table, except columns named 
 *                                   ‘id’ and ‘class’, will be used as features.
 * @param id_col_name               The name of the column containing an ID for each record.
 * @param class_col_name            The name of the column containing the labeled class. 
 * @param how2handle_missing_value  The way to handle missing value. The valid value 
 *                                  is 'explicit' or 'ignore'.
 * @param max_tree_depth            Specifies the maximum number of levels in the result DT 
 *                                  to avoid overgrown DTs. 
 * @param node_prune_threshold      The minimum percentage of the number of records required in a
 *                                  child node. It can't be NULL. The range of it is in [0.0, 1.0].
 *                                  This threshold only applies to the non-root nodes. Therefore,
 *                                  if its value is 1, then the trained tree only has one node (the root node);
 *                                  if its value is 0, then no nodes will be pruned by this parameter.
 * @param node_split_threshold      The minimum percentage of the number of records required in a
 *                                  node in order for a further split to be possible.
 *                                  It can't be NULL. The range of it is in [0.0, 1.0].
 *                                  If it's value is 1, then the trained tree only has two levels, since
 *                                  only the root node can grow; if its value is 0, then trees can grow
 *                                  extensively.
 * @param verbosity                 > 0 means this function runs in verbose mode.   
 * @param error_msg                 The reported error message when result_table_name is invalid.
 *
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__check_dt_common_params
    (
    split_criterion             TEXT,
    training_table_name         TEXT, 
    result_table_name           TEXT,
    continuous_feature_names    TEXT, 
    feature_col_names           TEXT, 
    id_col_name                 TEXT, 
    class_col_name              TEXT, 
    how2handle_missing_value    TEXT,
    max_tree_depth              INT,
    node_prune_threshold        FLOAT,
    node_split_threshold        FLOAT, 
    verbosity                   INT,
    error_msg                   TEXT
    ) 
RETURNS void AS $$
    PythonFunctionBodyOnly(`cart', `dt')
   
    dt.__check_dt_common_params(MADlibSchema, split_criterion, training_table_name, result_table_name,
        continuous_feature_names, feature_col_names, id_col_name, class_col_name, how2handle_missing_value,
        max_tree_depth, node_prune_threshold, node_split_threshold, verbosity, error_msg)
$$ LANGUAGE PLPYTHONU STABLE;


/*
 * @brief Get the name of the encoded table and the name of
 *        its meta table.
 * @param result_table_name   The name of the table where the 
 *                            resulting DT will be kept 
 * @param error_msg           The reported error message when the
 *                            length of result schema name plus
 *                            the length of result table name is
 *                            larger than 58.
 * 
 * @return A text array that contains two elements. The firest element
 *        is the encoded table name and the second is the meta table name.
 *                            
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__gen_enc_meta_names
    (
    result_table_name     TEXT,
    error_msg             TEXT
    ) 
RETURNS TEXT[] AS $$
    PythonFunctionBodyOnly(`cart', `dt')

    return dt.__gen_enc_meta_names(MADlibSchema, result_table_name, error_msg)
$$ LANGUAGE PLPYTHONU STABLE;


/*
 * @brief Validate if the provided columns are in the training table or not.
 *
 * @param training_table_name       The name of the table/view with the source data.
 * @param continuous_feature_names  A text array that contains all the continuous 
 *                                  features' names. 
 * @param feature_col_names         A text array that contains all the features' names.
 * @param id_col_name               The name of the column containing an ID for each record.
 * @param class_col_name            The name of the column containing the labeled class. 
 * @param features_per_node         The number of features to be considered when finding 
 *									a best split.
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__check_training_table
    (
    training_table_name         TEXT,
    continuous_feature_names    TEXT[], 
    feature_col_names           TEXT[], 
    id_col_name                 TEXT, 
    class_col_name              TEXT,
    features_per_node           INT
    ) 
RETURNS VOID AS $$
    PythonFunctionBodyOnly(`cart', `dt')

    dt.__check_training_table(MADlibSchema, training_table_name, continuous_feature_names, 
           feature_col_names, id_col_name, class_col_name, features_per_node)
$$ LANGUAGE PLPYTHONU STABLE;


/* @ brief If the training table is a valid encoded table, then we use it directly.
 *         If the training table is not encoded, then we invoke the encoding procedure
 *         to transform the training table. 
 *         With the encoded table, we call the tree grow engine to generate the final tree.
 *
 * @param dt_algo_name                The name of the algorithom. Currently, it's
 *                                    'C4.5' or 'RF'
 * @param split_criterion             This parameter specifies which split criterion 
 *                                    should be used for tree construction and 
 *                                    pruning. The valid values are infogain, 
 *                                    gainratio, and gini.
 * @param num_trees                   Total number of trees to be trained. 
 * @param features_per_node           Total number of features used to compute split 
 *                                    gain for each node. 
 * @param training_table_name         The name of the table/view with the source data. 
 * @param validation_table_name       The name of the validation table. 
 * @param tree_table_name             The name of the table where the resulting 
 *                                    DT/RF will be stored. 
 * @param continuous_feature_names    A comma-separated list of the names of features whose values 
 *                                    are continuous. The default is null, which means there are 
 *                                    no continuous features in the training table.
 * @param feature_col_names           A comma-separated list of the names of table columns, each of
 *                                    which defines a feature. The default value is null, which means 
 *                                    all the columns in the training table, except columns named 
 *                                   ‘id’ and ‘class’, will be used as features.
 * @param id_col_name                 The name of the column containing id of each point.  
 * @param class_col_name              The name of the column containing correct class 
 *                                    of each point.  
 * @param confidence_level            A statistical confidence interval of the 
 *                                    resubstitution error.  
 * @param how2handle_missing_value    The way to handle missing value. The valid value 
 *                                    is 'explicit' or 'ignore'.
 * @param max_tree_depth              Maximum decision tree depth.  
 * @param sampling_percentage         The percentage of records sampled to train a tree.
 *									  If it's NULL, 0.632 bootstrap will be used
 * @param sampling_needed             Whether enabling the sampling functionality.  
 * @param node_prune_threshold        Specifies the minimum number of samples required 
 *                                    in a child node.  
 * @param node_split_threshold        Specifies the minimum number of samples required 
 *                                    in a node in order for a further split   
 *                                    to be possible.  
 * @param error_msg                   The reported error message when the result table
 *                                    name is invalid.
 * @param verbosity                   > 0 means this function runs in verbose mode. 
 *
 * @return An instance of __train_result.
 *
 */
CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__encode_and_train_return_udt
    (
    dt_algo_name                TEXT,
    split_criterion             TEXT,
    num_trees                   INT,
    features_per_node           INT,
    training_table_name         TEXT,
    validation_table_name       TEXT,
    tree_table_name             TEXT,
    continuous_feature_names    TEXT,
    feature_col_names           TEXT,
    id_col_name                 TEXT,
    class_col_name              TEXT,
    confidence_level            FLOAT8,
    how2handle_missing_value    TEXT,
    max_tree_depth              INT,
    sampling_percentage         FLOAT8,
    sampling_needed             BOOL,
    node_prune_threshold        FLOAT8,
    node_split_threshold        FLOAT8,
    error_msg                   TEXT,
    verbosity                   INT
    )
RETURNS MADLIB_SCHEMA.__train_result
AS $$
    PythonFunctionBodyOnly(`cart', `dt')

    return dt.__encode_and_train(MADlibSchema, dt_algo_name, split_criterion, num_trees, features_per_node,
               training_table_name, validation_table_name, tree_table_name, continuous_feature_names, 
               feature_col_names, id_col_name, class_col_name, confidence_level, how2handle_missing_value,
               max_tree_depth, sampling_percentage, sampling_needed, node_prune_threshold, node_split_threshold,
               error_msg, verbosity
    )  
$$ LANGUAGE PLPYTHONU;


/* @ brief If the training table is a valid encoded table, then we use it directly.
 *         If the training table is not encoded, then we invoke the encoding procedure
 *         to transform the training table. 
 *         With the encoded table, we call the tree grow engine to generate the final tree.
 *
 * @param dt_algo_name                The name of the algorithom. Currently, it's
 *                                    'C4.5' or 'RF'
 * @param split_criterion             This parameter specifies which split criterion 
 *                                    should be used for tree construction and 
 *                                    pruning. The valid values are infogain, 
 *                                    gainratio, and gini.
 * @param num_trees                   Total number of trees to be trained. 
 * @param features_per_node           Total number of features used to compute split 
 *                                    gain for each node. 
 * @param training_table_name         The name of the table/view with the source data. 
 * @param validation_table_name       The name of the validation table. 
 * @param tree_table_name             The name of the table where the resulting 
 *                                    DT/RF will be stored. 
 * @param continuous_feature_names    A comma-separated list of the names of features whose values 
 *                                    are continuous. The default is null, which means there are 
 *                                    no continuous features in the training table.
 * @param feature_col_names           A comma-separated list of the names of table columns, each of
 *                                    which defines a feature. The default value is null, which means 
 *                                    all the columns in the training table, except columns named 
 *                                   ‘id’ and ‘class’, will be used as features.
 * @param id_col_name                 The name of the column containing id of each point.  
 * @param class_col_name              The name of the column containing correct class 
 *                                    of each point.  
 * @param confidence_level            A statistical confidence interval of the 
 *                                    resubstitution error.  
 * @param how2handle_missing_value    The way to handle missing value. The valid value 
 *                                    is 'explicit' or 'ignore'.
 * @param max_tree_depth              Maximum decision tree depth.  
 * @param sampling_percentage         The percentage of records sampled to train a tree.
 *									  If it's NULL, 0.632 bootstrap will be used
 * @param sampling_needed             Whether enabling the sampling functionality.  
 * @param node_prune_threshold        Specifies the minimum number of samples required 
 *                                    in a child node.  
 * @param node_split_threshold        Specifies the minimum number of samples required 
 *                                    in a node in order for a further split   
 *                                    to be possible.  
 * @param error_msg                   The reported error message when the result table
 *                                    name is invalid.
 * @param verbosity                   > 0 means this function runs in verbose mode. 
 *
 * @return An instance of __train_result.
 *
 */


CREATE OR REPLACE FUNCTION MADLIB_SCHEMA.__encode_and_train 
    (
    dt_algo_name                TEXT,
    split_criterion             TEXT,
    num_trees                   INT,
    features_per_node           INT,
    training_table_name         TEXT,
    validation_table_name       TEXT,
    tree_table_name             TEXT,
    continuous_feature_names    TEXT,
    feature_col_names           TEXT, 
    id_col_name                 TEXT, 
    class_col_name              TEXT, 
    confidence_level            FLOAT8,    
    how2handle_missing_value    TEXT,
    max_tree_depth              INT,
    sampling_percentage         FLOAT8,
    sampling_needed             BOOL,
    node_prune_threshold        FLOAT8,
    node_split_threshold        FLOAT8, 
    error_msg                   TEXT,
    verbosity                   INT,
OUT num_of_samples              BIGINT,   
OUT features_per_node           INT,
OUT num_tree_nodes              INT,
OUT max_tree_depth              INT,
OUT calc_acc_time               INTERVAL,
OUT calc_pre_time               INTERVAL,
OUT update_time                 INTERVAL,
OUT update_best                 INTERVAL,
OUT update_child                INTERVAL,
OUT update_nid                  INTERVAL,
OUT scv_acs_time                INTERVAL,
OUT prune_time                  INTERVAL    
    ) 
RETURNS RECORD AS $$
    PythonFunctionBodyOnly(`cart', `dt') 

    return dt.__encode_and_train(MADlibSchema, dt_algo_name, split_criterion, num_trees, features_per_node,
               training_table_name, validation_table_name, tree_table_name, continuous_feature_names,
               feature_col_names, id_col_name, class_col_name, confidence_level, how2handle_missing_value,
               max_tree_depth, sampling_percentage, sampling_needed, node_prune_threshold, node_split_threshold,
               error_msg, verbosity
    )
$$ LANGUAGE PLPYTHONU;
